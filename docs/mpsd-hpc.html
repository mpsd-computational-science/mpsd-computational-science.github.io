
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>2. MPSD HPC system &#8212; MPSD SSU Computational Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../_static/styles/bootstrap.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=796348d33e8b1d947c94" rel="stylesheet">
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94">
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/mpsd-hpc';</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. Software" href="software.html" />
    <link rel="prev" title="1. Overview computing services" href="overview-it.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="docsearch:language" content="en">
  </head>
  
  
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="180" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>

  
  <input type="checkbox" class="sidebar-toggle" name="__primary" id="__primary">
  <label class="overlay overlay-primary" for="__primary"></label>

  
  <input type="checkbox" class="sidebar-toggle" name="__secondary" id="__secondary">
  <label class="overlay overlay-secondary" for="__secondary"></label>

  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
      
<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
  </div>

  
  <nav class="bd-header navbar navbar-expand-lg bd-navbar" id="navbar-main"><div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
      <span class="fa-solid fa-bars"></span>
  </label>
  <div id="navbar-start">
    
    
  


<a class="navbar-brand logo" href="../index.html">

  
  
  
  
  
  
  

  
    <img src="../_static/computational-science-mpsd2.png" class="logo__image only-light" alt="Logo image">
    <img src="../_static/computational-science-mpsd2.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    
  </div>

  
  <div class="col-lg-9 navbar-header-items">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../about.html">
                        About
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="index.html">
                        Docs
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../events.html">
                        Events
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../gallery.html">
                        Gallery
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../support.html">
                        Support
                      </a>
                    </li>
                

                <li class="nav-item">
                  <a class="nav-link nav-external" href="https://www.mpsd.mpg.de/imprint">
                    Imprint
                  </a>
                </li>
                

                <li class="nav-item">
                  <a class="nav-link nav-external" href="https://www.mpsd.mpg.de/289505/dataprotection">
                    Privacy Policy
                  </a>
                </li>
                
    </ul>
</nav>
      </div>
      
    </div>

    <div id="navbar-end">
      
        <div class="navbar-end-item navbar-persistent--container">
          
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
        </div>
      
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>


  
  
    <div class="navbar-persistent--mobile">
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
    </div>
  

  
  <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
  </label>
  

</div>
  </nav>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        
  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../about.html">
                        About
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="index.html">
                        Docs
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../events.html">
                        Events
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../gallery.html">
                        Gallery
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../support.html">
                        Support
                      </a>
                    </li>
                

                <li class="nav-item">
                  <a class="nav-link nav-external" href="https://www.mpsd.mpg.de/imprint">
                    Imprint
                  </a>
                </li>
                

                <li class="nav-item">
                  <a class="nav-link nav-external" href="https://www.mpsd.mpg.de/289505/dataprotection">
                    Privacy Policy
                  </a>
                </li>
                
    </ul>
</nav>
      </div>
      
      </div>
    

    
    
    <div class="sidebar-header-items__end">
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
    
  </div>

  
  <div class="sidebar-start-items sidebar-primary__section">
    <div class="sidebar-start-items__item"><nav class="bd-links" id="bd-docs-nav" aria-label="Section navigation">
  <p class="bd-links__title" role="heading" aria-level="1">
    Section Navigation
  </p>
  <div class="bd-toc-item navbar-nav">
    <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="overview-it.html">1. Overview computing services</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">2. MPSD HPC system</a></li>
<li class="toctree-l1"><a class="reference internal" href="software.html">3. Software</a></li>
<li class="toctree-l1"><a class="reference internal" href="version-control.html">4. Version control</a></li>
<li class="toctree-l1"><a class="reference internal" href="research-data-management.html">5. Research Data Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="archive.html">6. Archiving research data</a></li>
<li class="toctree-l1"><a class="reference internal" href="keeper.html">7. Keeper</a></li>
<li class="toctree-l1"><a class="reference internal" href="zulip.html">8. Zulip</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">9. License</a></li>
<li class="toctree-l1"><a class="reference internal" href="changes.html">10. Changelog</a></li>
</ul>

  </div>
</nav>
    </div>
  </div>
  

  
  <div class="sidebar-end-items sidebar-primary__section">
    <div class="sidebar-end-items__item">
    </div>
  </div>

  
  <div id="rtd-footer-container"></div>

      </div>
      <main id="main-content" class="bd-main">
        
        
        <div class="bd-content">
          <div class="bd-article-container">
            
            <div class="bd-header-article">
                
            </div>
            
            
            <article class="bd-article" role="main">
              
  <section id="mpsd-hpc-system">
<span id="mpsd-hpc"></span><h1><span class="section-number">2. </span>MPSD HPC system<a class="headerlink" href="#mpsd-hpc-system" title="Permalink to this heading">#</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#login-nodes" id="id3">Login nodes</a></p></li>
<li><p><a class="reference internal" href="#job-submission" id="id4">Job submission</a></p>
<ul>
<li><p><a class="reference internal" href="#partitions" id="id5">Partitions</a></p></li>
<li><p><a class="reference internal" href="#slurm-default-values" id="id6">Slurm default values</a></p></li>
<li><p><a class="reference internal" href="#slurm-cpus" id="id7">Slurm CPUs</a></p></li>
<li><p><a class="reference internal" href="#interactive-use-of-hpc-nodes" id="id8">Interactive use of HPC nodes</a></p></li>
<li><p><a class="reference internal" href="#finding-about-about-my-jobs" id="id9">Finding about about my jobs</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#storage-and-quotas" id="id10">Storage and quotas</a></p></li>
<li><p><a class="reference internal" href="#software" id="id11">Software</a></p>
<ul>
<li><p><a class="reference internal" href="#tldr" id="id12">TLDR</a></p></li>
<li><p><a class="reference internal" href="#initial-setup" id="id13">Initial setup</a></p></li>
<li><p><a class="reference internal" href="#loading-specific-packages" id="id14">Loading specific packages</a></p></li>
<li><p><a class="reference internal" href="#python" id="id15">Python</a></p></li>
<li><p><a class="reference internal" href="#jupyter-notebooks" id="id16">Jupyter notebooks</a></p></li>
<li><p><a class="reference internal" href="#matlab" id="id17">Matlab</a></p></li>
<li><p><a class="reference internal" href="#loading-a-toolchain-to-compile-octopus" id="id18">Loading a toolchain to compile octopus</a></p></li>
<li><p><a class="reference internal" href="#compiling-custom-code" id="id19">Compiling custom code</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#example-batch-scripts" id="id20">Example batch scripts</a></p>
<ul>
<li><p><a class="reference internal" href="#mpi" id="id21">MPI</a></p></li>
<li><p><a class="reference internal" href="#mpi-openmp" id="id22">MPI + OpenMP</a></p></li>
<li><p><a class="reference internal" href="#openmp" id="id23">OpenMP</a></p></li>
<li><p><a class="reference internal" href="#python-with-numpy-or-multiprocessing" id="id24">Python with numpy or multiprocessing</a></p></li>
<li><p><a class="reference internal" href="#single-core-job" id="id25">Single-core job</a></p></li>
<li><p><a class="reference internal" href="#serial-python" id="id26">Serial Python</a></p></li>
<li><p><a class="reference internal" href="#gpu-jobs" id="id27">GPU jobs</a></p></li>
</ul>
</li>
</ul>
</nav>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>We are working on an upgrade of software stack and documentation on the local
HPC cluster. The cluster will be unavailable until the end of this maintenance.</p>
</div>
<p>For the Raven and Ada machine, please check <a class="reference internal" href="overview-it.html"><span class="doc">Overview computing services</span></a>.</p>
<section id="login-nodes">
<h2><a class="toc-backref" href="#id3" role="doc-backlink"><span class="section-number">2.1. </span>Login nodes</a><a class="headerlink" href="#login-nodes" title="Permalink to this heading">#</a></h2>
<p>Login nodes are <code class="docutils literal notranslate"><span class="pre">mpsd-hpc-login1.desy.de</span></code> and
<code class="docutils literal notranslate"><span class="pre">mpsd-hpc-login2.desy.de</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>During the first login your home directory will be created. This could take up to a minute. Please be patient.</p>
</div>
</section>
<section id="job-submission">
<h2><a class="toc-backref" href="#id4" role="doc-backlink"><span class="section-number">2.2. </span>Job submission</a><a class="headerlink" href="#job-submission" title="Permalink to this heading">#</a></h2>
<p>Job submission is via <a class="reference external" href="https://slurm.schedmd.com">Slurm</a>.</p>
<p>Example slurm submission jobs are available below
(<a class="reference internal" href="#examplebatchscripts"><span class="std std-ref">Example batch scripts</span></a>).</p>
<section id="partitions">
<h3><a class="toc-backref" href="#id5" role="doc-backlink"><span class="section-number">2.2.1. </span>Partitions</a><a class="headerlink" href="#partitions" title="Permalink to this heading">#</a></h3>
<p>The following partitions are available to all (partial output from
<code class="docutils literal notranslate"><span class="pre">sinfo</span></code>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PARTITION</span>  <span class="n">AVAIL</span>  <span class="n">TIMELIMIT</span>  <span class="n">NODES</span> <span class="n">NODELIST</span>
<span class="n">bigmem</span>        <span class="n">up</span> <span class="mi">7</span><span class="o">-</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>      <span class="mi">8</span> <span class="n">mpsd</span><span class="o">-</span><span class="n">hpc</span><span class="o">-</span><span class="n">hp</span><span class="o">-</span><span class="p">[</span><span class="mi">001</span><span class="o">-</span><span class="mi">008</span><span class="p">]</span>
<span class="n">gpu</span>           <span class="n">up</span> <span class="mi">7</span><span class="o">-</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>      <span class="mi">2</span> <span class="n">mpsd</span><span class="o">-</span><span class="n">hpc</span><span class="o">-</span><span class="n">gpu</span><span class="o">-</span><span class="p">[</span><span class="mi">001</span><span class="o">-</span><span class="mi">002</span><span class="p">]</span>
<span class="n">gpu</span><span class="o">-</span><span class="n">ayyer</span>     <span class="n">up</span> <span class="mi">7</span><span class="o">-</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>      <span class="mi">3</span> <span class="n">mpsd</span><span class="o">-</span><span class="n">hpc</span><span class="o">-</span><span class="n">gpu</span><span class="o">-</span><span class="p">[</span><span class="mi">003</span><span class="o">-</span><span class="mi">005</span><span class="p">]</span>
<span class="n">public</span><span class="o">*</span>       <span class="n">up</span> <span class="mi">7</span><span class="o">-</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>     <span class="mi">49</span> <span class="n">mpsd</span><span class="o">-</span><span class="n">hpc</span><span class="o">-</span><span class="n">ibm</span><span class="o">-</span><span class="p">[</span><span class="mi">001</span><span class="o">-</span><span class="mi">030</span><span class="p">,</span><span class="mi">035</span><span class="o">-</span><span class="mi">036</span><span class="p">,</span><span class="mi">043</span><span class="o">-</span><span class="mi">049</span><span class="p">,</span><span class="mi">053</span><span class="o">-</span><span class="mi">062</span><span class="p">]</span>
</pre></div>
</div>
<p>Please use the machines in the <code class="docutils literal notranslate"><span class="pre">gpu</span></code> partition only if your code
supports nvidia-cuda.</p>
<p>Hardware resources per node:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">public</span></code> partitions</p>
<ul>
<li><p>8 physical cores (16 with hyperthreading)</p></li>
<li><p>64GB RAM</p></li>
<li><p>at most 2 nodes can be used for multi-node jobs (as the 10GB
ethernet for MPI has a relatively high latency)</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">bigmem</span></code></p>
<ul>
<li><p>96 physical cores (192 with hyperthreading)</p></li>
<li><p>2T RAM</p></li>
<li><p>fast FDR infiniband for MPI communication</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpu</span></code></p>
<ul>
<li><p>16 physical cores (32 with hyperthreading)</p></li>
<li><p>1.5T RAM</p></li>
<li><p>fast FDR infiniband for MPI communication</p></li>
<li><p>8 Tesla V100 GPUs</p></li>
<li><p>at most 2 nodes can be used for multi-node jobs (as the 10GB
ethernet for MPI has a relatively high latency)</p></li>
</ul>
</li>
</ul>
<p>The maximum runtime for any job is 7 days (that is 10080 minutes).</p>
</section>
<section id="slurm-default-values">
<h3><a class="toc-backref" href="#id6" role="doc-backlink"><span class="section-number">2.2.2. </span>Slurm default values</a><a class="headerlink" href="#slurm-default-values" title="Permalink to this heading">#</a></h3>
<p>Slurm defaults are an execution time of 1 hour, one task, one CPU, a
fraction of the memory of the allocated node, and the <code class="docutils literal notranslate"><span class="pre">public</span></code>
partition.</p>
<p>Nodes by default are shared between multiple users (i.e. use is not
exclusive, unless requested).</p>
<p>Logging onto nodes via <code class="docutils literal notranslate"><span class="pre">ssh</span></code> is only possible once the nodes are
allocated (either via <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> when the job starts, of using
<code class="docutils literal notranslate"><span class="pre">salloc</span></code> for interactive use, see
<a class="reference internal" href="#interactive-use-of-hpc-nodes"><span class="std std-ref">Interactive use of HPC nodes</span></a>). This avoids accidental over-use of
resources and enables energy saving measures (such as switching compute
nodes off automatically if they are not in use).</p>
</section>
<section id="slurm-cpus">
<h3><a class="toc-backref" href="#id7" role="doc-backlink"><span class="section-number">2.2.3. </span>Slurm CPUs</a><a class="headerlink" href="#slurm-cpus" title="Permalink to this heading">#</a></h3>
<p>Note that a “CPU” in Slurm terminology is a <a class="reference external" href="https://slurm.schedmd.com/faq.html#cpu_count">computational core (or a
thread if hyperthreading is
configured)</a>, whereas
in the language in daily use we would consider a (populated) CPU
(socket) to be the device holding many computational cores (such as 16
cores per CPU socket or so). We use the Slurm convention in this
document as do the slurm commands.</p>
</section>
<section id="interactive-use-of-hpc-nodes">
<span id="id1"></span><h3><a class="toc-backref" href="#id8" role="doc-backlink"><span class="section-number">2.2.4. </span>Interactive use of HPC nodes</a><a class="headerlink" href="#interactive-use-of-hpc-nodes" title="Permalink to this heading">#</a></h3>
<p>For production computation, we typically write a batch file (see
<a class="reference internal" href="#examplebatchscripts"><span class="std std-ref">Example batch scripts</span></a>), and submit these using the <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>
command.</p>
<p>Sometimes, it can be helpful to login into an HPC node for example to
compile software or run interactive tests. The command to use in this
case is <code class="docutils literal notranslate"><span class="pre">salloc</span></code>.</p>
<p>For example, requesting a job with all default settings:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>user@mpsd-hpc-login1:~$<span class="w"> </span>salloc
salloc:<span class="w"> </span>Granted<span class="w"> </span>job<span class="w"> </span>allocation<span class="w"> </span><span class="m">1272</span>
user@mpsd-hpc-ibm-058:~$
</pre></div>
</div>
<p>We can see from the prompt (<code class="docutils literal notranslate"><span class="pre">user&#64;mpsd-hpc-ibm-058:~$</span></code>) that the Slurm
system has allocated node <code class="docutils literal notranslate"><span class="pre">mpsd-hpc-ibm-058</span></code> to us.</p>
<p>We can use the <code class="docutils literal notranslate"><span class="pre">mpsd-show-job-resources</span></code> command to check some details
of the allocation:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>user@mpsd-hpc-ibm-058:~$<span class="w"> </span>mpsd-show-job-resources
<span class="w"> </span><span class="m">345415</span><span class="w"> </span>Nodes:<span class="w"> </span>mpsd-hpc-ibm-058
<span class="w"> </span><span class="m">345415</span><span class="w"> </span>Local<span class="w"> </span>Node:<span class="w"> </span>mpsd-hpc-ibm-058

<span class="w"> </span><span class="m">345415</span><span class="w"> </span>CPUSET:<span class="w"> </span><span class="m">0</span>
<span class="w"> </span><span class="m">345415</span><span class="w"> </span>MEMORY:<span class="w"> </span><span class="m">3500</span><span class="w"> </span>M
</pre></div>
</div>
<p>We can finish our interactive session by typing <code class="docutils literal notranslate"><span class="pre">exit</span></code>.</p>
<p>If we desire exclusive use of a node (i.e. not shared with others), we
can use <code class="docutils literal notranslate"><span class="pre">salloc</span> <span class="pre">--exclusive</span></code> (he we request a session time of 120
minutes):</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>user@mpsd-hpc-login2:~$<span class="w"> </span>salloc<span class="w">  </span>--exclusive<span class="w"> </span>--time<span class="o">=</span><span class="m">120</span>
salloc:<span class="w"> </span>Granted<span class="w"> </span>job<span class="w"> </span>allocation<span class="w"> </span><span class="m">1279</span>
user@mpsd-hpc-ibm-061:~$<span class="w"> </span>mpsd-show-job-resources
<span class="w">  </span><span class="m">65911</span><span class="w"> </span>Nodes:<span class="w"> </span>mpsd-hpc-ibm-061
<span class="w">  </span><span class="m">65911</span><span class="w"> </span>Local<span class="w"> </span>Node:<span class="w"> </span>mpsd-hpc-ibm-061

<span class="w">  </span><span class="m">65911</span><span class="w"> </span>CPUSET:<span class="w"> </span><span class="m">0</span>-15
<span class="w">  </span><span class="m">65911</span><span class="w"> </span>MEMORY:<span class="w"> </span><span class="m">56000</span><span class="w"> </span>M
</pre></div>
</div>
<p>We can see (in the output above) that all 16 CPUs of the node are
allocated to us.</p>
<p>Assume we need 16 CPUs and 10GB of RAM for our interactive session (the
16 CPUs corresponds to the number of OpenMP threads, see
<a class="reference internal" href="#example-openmp"><span class="std std-ref">OpenMP</span></a>):</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>user@mpsd-hpc-login1:~$<span class="w"> </span>salloc<span class="w"> </span>--mem<span class="o">=</span><span class="m">10000</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">16</span>
salloc:<span class="w"> </span>Granted<span class="w"> </span>job<span class="w"> </span>allocation<span class="w"> </span><span class="m">1273</span>
user@mpsd-hpc-ibm-058:~$<span class="w"> </span>mpsd-show-job-resources
<span class="w"> </span><span class="m">345446</span><span class="w"> </span>Nodes:<span class="w"> </span>mpsd-hpc-ibm-058
<span class="w"> </span><span class="m">345446</span><span class="w"> </span>Local<span class="w"> </span>Node:<span class="w"> </span>mpsd-hpc-ibm-058

<span class="w"> </span><span class="m">345446</span><span class="w"> </span>CPUSET:<span class="w"> </span><span class="m">0</span>-15
<span class="w"> </span><span class="m">345446</span><span class="w"> </span>MEMORY:<span class="w"> </span><span class="m">10000</span><span class="w"> </span>M
user@mpsd-hpc-ibm-058:~
</pre></div>
</div>
<p>If we execute MPI programs, we need to specify the number of nodes (a
node is a computer node, with typically one, two or four CPU sockets),
and how many (MPI) tasks (=processes) we want to run on that node.
Imagine we ask for two nodes, and want to run 4 MPI processes on each:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>user@mpsd-hpc-login1:~$<span class="w"> </span>salloc<span class="w"> </span>--nodes<span class="o">=</span><span class="m">2</span><span class="w"> </span>--tasks-per-node<span class="o">=</span><span class="m">4</span>
salloc:<span class="w"> </span>Granted<span class="w"> </span>job<span class="w"> </span>allocation<span class="w"> </span><span class="m">1276</span>
user@mpsd-hpc-ibm-058:~$<span class="w"> </span>mpsd-show-job-resources
<span class="w"> </span><span class="m">345591</span><span class="w"> </span>Nodes:<span class="w"> </span>mpsd-hpc-ibm-<span class="o">[</span><span class="m">058</span>-059<span class="o">]</span>
<span class="w"> </span><span class="m">345591</span><span class="w"> </span>Local<span class="w"> </span>Node:<span class="w"> </span>mpsd-hpc-ibm-058

<span class="w"> </span><span class="m">345591</span><span class="w"> </span>CPUSET:<span class="w"> </span><span class="m">0</span>-3
<span class="w"> </span><span class="m">345591</span><span class="w"> </span>MEMORY:<span class="w"> </span><span class="m">14000</span><span class="w"> </span>M
user@mpsd-hpc-ibm-058:~$<span class="w"> </span>srun<span class="w"> </span>hostname
mpsd-hpc-ibm-059
mpsd-hpc-ibm-059
mpsd-hpc-ibm-059
mpsd-hpc-ibm-059
mpsd-hpc-ibm-058
mpsd-hpc-ibm-058
mpsd-hpc-ibm-058
mpsd-hpc-ibm-058
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">srun</span></code> command starts the execution of our (MPI) tasks. We use the
<code class="docutils literal notranslate"><span class="pre">hostname</span></code> command above and can see that we have 4 of these commands
run on each node.</p>
</section>
<section id="finding-about-about-my-jobs">
<h3><a class="toc-backref" href="#id9" role="doc-backlink"><span class="section-number">2.2.5. </span>Finding about about my jobs</a><a class="headerlink" href="#finding-about-about-my-jobs" title="Permalink to this heading">#</a></h3>
<p>There are multiple ways of finding out about your slurm jobs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">squeue</span> <span class="pre">--me</span></code> lists only your jobs (see below for output)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mpsd-show-job-resources</span></code> can be used ‘inside’ the job (to verify
hardware allocation is as desired)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scontrol</span> <span class="pre">show</span> <span class="pre">job</span> <span class="pre">JOBID</span></code> provides a lot of detail</p></li>
</ul>
<p>Example: We request 2 nodes, with 4 tasks (and by default one CPU per
task)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>fangohr@mpsd-hpc-login1:~$ salloc --nodes=2 --tasks-per-node=4
salloc: Granted job allocation 1276

fangohr@mpsd-hpc-login2:~$ squeue --me
  JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
   1276    public interact  fangohr  R      11:37      2 mpsd-hpc-ibm-[058-059]

fangohr@mpsd-hpc-ibm-058:~$ mpsd-show-job-resources
 345591 Nodes: mpsd-hpc-ibm-[058-059]
 345591 Local Node: mpsd-hpc-ibm-058

 345591 CPUSET: 0-3
 345591 MEMORY: 14000 M

fangohr@mpsd-hpc-login2:~$ scontrol show job 1276
JobId=1276 JobName=interactive
   UserId=fangohr(28479) GroupId=cfel(3512) MCS_label=N/A
   &lt;...&gt;
   RunTime=00:14:08 TimeLimit=01:00:00 TimeMin=N/A
   Partition=public AllocNode:Sid=mpsd-hpc-login1.desy.de:3116660
   NodeList=mpsd-hpc-ibm-[058-059]
   BatchHost=mpsd-hpc-ibm-058
   NumNodes=2 NumCPUs=8 NumTasks=8 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
   TRES=cpu=8,mem=28000M,node=2,billing=8
   Socks/Node=* NtasksPerN:B:S:C=4:0:*:1 CoreSpec=*
   MinCPUsNode=4 MinMemoryCPU=3500M MinTmpDiskNode=0
   &lt;...&gt;
</pre></div>
</div>
</section>
</section>
<section id="storage-and-quotas">
<h2><a class="toc-backref" href="#id10" role="doc-backlink"><span class="section-number">2.3. </span>Storage and quotas</a><a class="headerlink" href="#storage-and-quotas" title="Permalink to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">/home/&lt;username&gt;</span></code> (<code class="docutils literal notranslate"><span class="pre">$HOME</span></code>)</p>
<ul class="simple">
<li><p>home file system for code and scripts</p></li>
<li><p>user quota (storage limit): 100 GB</p></li>
<li><p>quota can be checked with <code class="docutils literal notranslate"><span class="pre">df</span> <span class="pre">-h</span> <span class="pre">$HOME</span></code></p></li>
<li><p>regular backups</p></li>
<li><p>users have access to the backup of their data under
<code class="docutils literal notranslate"><span class="pre">$HOME/.zfs/snapshots</span></code></p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">/scratch/&lt;username&gt;</span></code></p>
<ul>
<li><p>scratch file system for simulation output and other temporary data</p></li>
<li><p>there are no backups for <code class="docutils literal notranslate"><span class="pre">/scratch</span></code>: hardware error or human error
can lead to data loss.</p></li>
<li><p>there is no individual quota, but a policy to manage overall usage:</p>
<p>If <code class="docutils literal notranslate"><span class="pre">/scratch</span></code> fills up, the cluster becomes unusable. Should this
happen, we will make space available through the following actions:</p>
<ol class="arabic simple">
<li><p>purchase and installation of additional hardware to increase
storage available <code class="docutils literal notranslate"><span class="pre">/scratch</span></code> (if funding and other constraints
allow this)</p></li>
<li><p>ask users to voluntarily reduce their usage of <code class="docutils literal notranslate"><span class="pre">/scratch</span></code> (by,
for example, deleting some data, or archiving completed projects
elsewhere)</p></li>
<li><p>if 1. and 2. do not resolve the situation, a script will be
started that deletes some of the files on <code class="docutils literal notranslate"><span class="pre">/scratch</span></code> (starting
with the oldest files). Notice will be given of this procedure.</p></li>
</ol>
</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">/scratch</span></code> is not available yet. (Last update: 10 Feb 2023)</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The backup of the old HPC installation is available under
<code class="docutils literal notranslate"><span class="pre">/backup/&lt;username&gt;</span></code> on the login nodes. Users can copy their existing data to
the new home and scratch directories. No data will be transfered automatically.</p>
</div>
</section>
<section id="software">
<span id="mpsd-hpc-software"></span><h2><a class="toc-backref" href="#id11" role="doc-backlink"><span class="section-number">2.4. </span>Software</a><a class="headerlink" href="#software" title="Permalink to this heading">#</a></h2>
<p>The software on the MPSD HPC system is provided via environment modules.
This facilitates providing different versions of the same software. The
software is organised in a hierarchical structure.</p>
<p>First, you need to decide which MPSD software environment <strong>version</strong>
you need. These are named according to calendar years: the first one in
2023 is `23a`. In preparation of that version, there is a `dev-23a`
which is used until `23a` is completed. In the examples below, this is
always `dev-23a`. We select that version using the `mpsd-modules`
command, for example `mpsd-modules dev-23a`.</p>
<p>In order to use a module we first have to load a base compiler and MPI.
That way we can choose between different compilers and MPI
implementations for a software. More details are given below.</p>
<p>From a high-level perspective, the required steps to use a particular
module are:</p>
<ol class="arabic simple">
<li><p>Activate the MPSD software environment version of modules</p></li>
<li><p>Search for the module to find available versions and required base
modules</p></li>
<li><p>Load required base modules (such as a compiler)</p></li>
<li><p>Load the desired module</p></li>
</ol>
<section id="tldr">
<h3><a class="toc-backref" href="#id12" role="doc-backlink"><span class="section-number">2.4.1. </span>TLDR</a><a class="headerlink" href="#tldr" title="Permalink to this heading">#</a></h3>
<ul>
<li><p>Modules are organised in a hierarchical structure with compiler and
MPI implementation as base modules.</p></li>
<li><p>Modules may be compiled with different feature sets. Use
<code class="docutils literal notranslate"><span class="pre">mpsd-modules</span></code> for switching.</p>
<ul>
<li><p>a generic feature set (runs on all nodes), activated by default</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mpsd</span><span class="o">-</span><span class="n">modules</span> <span class="n">dev</span><span class="o">-</span><span class="mi">23</span><span class="n">a</span>
</pre></div>
</div>
</li>
<li><p>architecture-dependent feature sets (depending on the CPU micro
architecture <code class="docutils literal notranslate"><span class="pre">$MPSD_MICROARCH</span></code> of the nodes)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>mpsd-modules -m $MPSD_MICROARCH dev-23a
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>To subsequently find and load modules:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">avail</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span> <span class="pre">&lt;module-name&gt;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">&lt;module1&gt;</span> <span class="pre">[&lt;module2&gt;</span> <span class="pre">...]</span></code></p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">toolchain</span></code> modules replicate <a class="reference external" href="https://docs.easybuild.io/en/latest/Common-toolchains.html">easybuild
toolchains</a>
and additionally contain all modules required to compile octopus.</p></li>
<li><p>Configure scripts to compile octopus are available under
<code class="docutils literal notranslate"><span class="pre">/opt_mpsd/linux-debian11/dev-23a/spack-environments/octopus/</span></code>.</p></li>
</ul>
</section>
<section id="initial-setup">
<h3><a class="toc-backref" href="#id13" role="doc-backlink"><span class="section-number">2.4.2. </span>Initial setup</a><a class="headerlink" href="#initial-setup" title="Permalink to this heading">#</a></h3>
<p>The MPSD HPC system consists of a heterogeneous set of compute nodes
with different CPU features. This is reflected in the available software
stack by providing both a <em>generic</em> set of modules that can be used on
all nodes as well as <em>specialised</em> sets of modules for the different
available (hardware) micro architectures. The latter will only run on
certain nodes.</p>
<p>A versioning scheme is used for the MPSD software environment to improve
reproducibility. Currently, all software is available in the <code class="docutils literal notranslate"><span class="pre">dev-23a</span></code>
release (i.e. the first release in 2023). Additional modules will be
added to this environment as long as they do not break anything.
Therefore, users should always specify the version of the modules they
use (even if only a single version is available). A new release will be
made if any addition/change would break backwards compatibility.</p>
<p>This heterogeneous setup makes it necessary to first add an additional
path where module files can be found. To activate the different sets of
modules we can use <code class="docutils literal notranslate"><span class="pre">mpsd_modules</span></code>. The function takes two arguments:
the <em>release number</em> (of the MPSD software environment, mandatory) and
the <em>feature set</em> (optional, the <em>generic</em> set is used by default).
Calling <code class="docutils literal notranslate"><span class="pre">mpsd_modules</span></code> without arguments will show help and list
available options.</p>
<p>To demonstrate the use of <code class="docutils literal notranslate"><span class="pre">mpsd_modules</span></code> we activate the <code class="docutils literal notranslate"><span class="pre">generic</span></code>
module set of the current software environment <code class="docutils literal notranslate"><span class="pre">dev-23a</span></code>. These
modules can be used on all HPC nodes.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>~$<span class="w"> </span>mpsd-modules<span class="w"> </span>dev-23a
</pre></div>
</div>
<p>Now, we can list available modules. At the time of writing this produces
the following output (truncated):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>~$<span class="w"> </span>module<span class="w"> </span>avail

------------<span class="w"> </span>/opt_mpsd/linux-debian11/develop/sandybridge/lmod/Core<span class="w"> </span>-------------
<span class="w">   </span>gcc/10.3.0<span class="w">                       </span>toolchains/foss2021a-mpi
<span class="w">   </span>toolchains/foss2021a-cuda-mpi<span class="w">    </span>toolchains/foss2021a-serial<span class="w"> </span><span class="o">(</span>D<span class="o">)</span>

-----------------------<span class="w"> </span>/usr/share/lmod/lmod/modulefiles<span class="w"> </span>-----------------------
<span class="w">   </span>Core/lmod<span class="w">    </span>Core/settarg<span class="w"> </span><span class="o">(</span>D<span class="o">)</span>

------------------------<span class="w"> </span>/usr/share/modules/modulefiles<span class="w"> </span>------------------------
<span class="w">   </span>mathematica<span class="w">    </span>mathematica12p2<span class="w">    </span>matlab<span class="w">    </span>matlab2021b

<span class="w">  </span>Where:
<span class="w">   </span>D:<span class="w">  </span>Default<span class="w"> </span>Module
</pre></div>
</div>
<p>We can only see a small number of modules. The reason for this is the
hierarchical structure mentioned before. The majority of modules are
only visible once we load a compiler (and depending on the package an
MPI implementation).</p>
<p>We can load a compiler and again list available modules. Now many more
are available:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>~$<span class="w"> </span>module<span class="w"> </span>load<span class="w"> </span>gcc/10.3.0
~$<span class="w"> </span>module<span class="w"> </span>avail

---------<span class="w"> </span>/opt_mpsd/linux-debian11/develop/sandybridge/lmod/gcc/10.3.0<span class="w"> </span>----------
<span class="w">   </span>autoconf-archive/2022.02.11<span class="w">               </span>libsigsegv/2.13
<span class="w">   </span>autoconf/2.71<span class="w">                             </span>libtiff/4.4.0
<span class="w">   </span>automake/1.16.3<span class="w">                           </span>libtool/2.4.6
<span class="w">   </span>bdftopcf/1.0.5<span class="w">                            </span>libvdwxc/0.4.0
<span class="w">   </span>berkeley-db/18.1.40<span class="w">                       </span>libxc/4.3.4
<span class="w">   </span>berkeleygw/2.1<span class="w">                            </span>libxfont/1.5.2
<span class="w">   </span>binutils/2.36.1<span class="w">                           </span>libxml2/2.10.1
<span class="w">   </span>bison/3.8.2<span class="w">                               </span>lz4/1.9.4
<span class="w">   </span>boost/1.80.0<span class="w">                              </span>m4.1.1.19
<span class="w">   </span>bzip2/1.0.8<span class="w">                               </span>meson/0.63.3
<span class="w">   </span>ca-certificates-mozilla/2022-10-11<span class="w">        </span>metis/5.1.0
<span class="w">   </span>cgal/5.0.3<span class="w">                                </span>mkfontdir/1.0.7
<span class="w">   </span>cmake/3.24.3<span class="w">                              </span>mkfontscale/1.1.2
<span class="w">   </span>cuda/11.4.4<span class="w">                               </span>mpfr/4.1.0
<span class="w">   </span>curl/7.85.0<span class="w">                               </span>munge/0.5.15
<span class="w">   </span>diffutils/3.8<span class="w">                             </span>nasm/2.15.05
<span class="w">   </span>eigen/3.4.0<span class="w">                               </span>ncurses/6.3
<span class="w">   </span>expat/2.4.8<span class="w">                               </span>nfft/3.4.1
<span class="w">   </span>fftw/3.3.9<span class="w">                                </span>ninja/1.11.1
<span class="w">   </span>findutils/4.9.0<span class="w">                           </span>nlopt/2.7.0
<span class="w">   </span>flex/2.6.3<span class="w">                                </span>numactl/2.0.14
<span class="w">   </span>flexiblas/3.0.4<span class="w">                           </span>openmpi/4.1.1-cuda-11.4.4
<span class="w">   </span>font-util/1.3.2<span class="w">                           </span>openmpi/4.1.1<span class="w">             </span><span class="o">(</span>D<span class="o">)</span>
<span class="w">   </span>fontconfig/2.13.94<span class="w">                        </span>openssh/9.1p1
<span class="w">   </span>fontsproto/2.1.3<span class="w">                          </span>openssl/1.1.1s
<span class="w">   </span>freetype/2.11.1<span class="w">                           </span>pcre/8.45
<span class="w">   </span>gawk/5.1.1<span class="w">                                </span>pcre2/10.39
<span class="w">   </span>gdbm/1.23<span class="w">                                 </span>perl/5.36.0
<span class="w">   </span>gdrcopy/2.3<span class="w">                               </span>pigz/2.7
<span class="w">   </span>gettext/0.21.1<span class="w">                            </span>pkgconf/1.8.0
<span class="w">   </span>glib/2.74.1<span class="w">                               </span>pmix/4.1.2-cuda-11.4.4
<span class="w">   </span>gmp/6.2.1<span class="w">                                 </span>pmix/4.1.2<span class="w">                </span><span class="o">(</span>D<span class="o">)</span>
<span class="w">   </span>gperf/3.1<span class="w">                                 </span>py-cython/0.29.32
<span class="w">   </span>gsl/2.7<span class="w">                                   </span>py-docutils/0.19
<span class="w">   </span>hdf5/1.12.2<span class="w">                               </span>py-numpy/1.23.4
<span class="w">   </span>hwloc/2.8.0-cuda-11.4.4<span class="w">                   </span>py-pip/22.2.2
<span class="w">   </span>hwloc/2.8.0<span class="w">                        </span><span class="o">(</span>D<span class="o">)</span><span class="w">    </span>py-setuptools/59.4.0
<span class="w">   </span>json-c/0.16<span class="w">                               </span>py-wheel/0.37.1
<span class="w">   </span>knem/1.1.4-cuda-11.4.4<span class="w">                    </span>python/3.9.5
<span class="w">   </span>knem/1.1.4<span class="w">                         </span><span class="o">(</span>D<span class="o">)</span><span class="w">    </span>rdma-core/41.0
<span class="w">   </span>krb5/1.19.3<span class="w">                               </span>readline/8.1.2
<span class="w">   </span>libbsd/0.11.5<span class="w">                             </span>slurm/21-08-8-2
<span class="w">   </span>libedit/3.1-20210216<span class="w">                      </span>sparskit/develop
<span class="w">   </span>libevent/2.1.12<span class="w">                           </span>sqlite/3.39.4
<span class="w">   </span>libffi/3.4.2<span class="w">                              </span>swig/4.0.2
<span class="w">   </span>libfontenc/1.1.3<span class="w">                          </span>tar/1.34
<span class="w">   </span>libfuse/3.11.0<span class="w">                            </span>texinfo/6.5
<span class="w">   </span>libgcrypt/1.10.1<span class="w">                          </span>ucx/1.13.1-cuda-11.4.4
<span class="w">   </span>libgd/2.2.4<span class="w">                               </span>ucx/1.13.1<span class="w">                </span><span class="o">(</span>D<span class="o">)</span>
<span class="w">   </span>libgpg-error/1.46<span class="w">                         </span>util-linux-uuid/2.38.1
<span class="w">   </span>libiconv/1.16<span class="w">                             </span>util-macros/1.19.3
<span class="w">   </span>libjpeg-turbo/2.1.3<span class="w">                       </span>xproto/7.0.31
<span class="w">   </span>libmd/1.0.4<span class="w">                               </span>xtrans/1.3.5
<span class="w">   </span>libnl/3.3.0<span class="w">                               </span>xz/5.2.7
<span class="w">   </span>libpciaccess/0.16<span class="w">                         </span>zlib/1.2.13
<span class="w">   </span>libpng/1.6.37<span class="w">                             </span>zstd/1.5.2

------------<span class="w"> </span>/opt_mpsd/linux-debian11/develop/sandybridge/lmod/Core<span class="w"> </span>-------------
<span class="w">   </span>gcc/10.3.0<span class="w">                    </span><span class="o">(</span>L<span class="o">)</span><span class="w">    </span>toolchains/foss2021a-mpi
<span class="w">   </span>toolchains/foss2021a-cuda-mpi<span class="w">        </span>toolchains/foss2021a-serial<span class="w"> </span><span class="o">(</span>D<span class="o">)</span>

-----------------------<span class="w"> </span>/usr/share/lmod/lmod/modulefiles<span class="w"> </span>-----------------------
<span class="w">   </span>Core/lmod<span class="w">    </span>Core/settarg<span class="w"> </span><span class="o">(</span>D<span class="o">)</span>

------------------------<span class="w"> </span>/usr/share/modules/modulefiles<span class="w"> </span>------------------------
<span class="w">   </span>mathematica<span class="w"> </span><span class="o">(</span>L<span class="o">)</span><span class="w">    </span>mathematica12p2<span class="w"> </span><span class="o">(</span>L<span class="o">)</span><span class="w">    </span>matlab<span class="w"> </span><span class="o">(</span>L<span class="o">)</span><span class="w">    </span>matlab2021b<span class="w"> </span><span class="o">(</span>L<span class="o">)</span>

<span class="w">  </span>Where:
<span class="w">   </span>L:<span class="w">  </span>Module<span class="w"> </span>is<span class="w"> </span>loaded
<span class="w">   </span>D:<span class="w">  </span>Default<span class="w"> </span>Module
</pre></div>
</div>
<p>We now unload all loaded modules:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>~$<span class="w"> </span>module<span class="w"> </span>purge
</pre></div>
</div>
</section>
<section id="loading-specific-packages">
<span id="id2"></span><h3><a class="toc-backref" href="#id14" role="doc-backlink"><span class="section-number">2.4.3. </span>Loading specific packages</a><a class="headerlink" href="#loading-specific-packages" title="Permalink to this heading">#</a></h3>
<p>To find a specific package we can use the <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span></code> command.
Without extra arguments this would list all modules. We can search for a
specific module by adding the module name.</p>
<p>As an example we can search for <code class="docutils literal notranslate"><span class="pre">octopus</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>~$<span class="w"> </span>module<span class="w"> </span>spider<span class="w"> </span>octopus
</pre></div>
</div>
<p>We see that different versions of <code class="docutils literal notranslate"><span class="pre">octopus</span></code> are available. We have to
specify a particular version in order to get more information on how to
load the module:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>~$<span class="w"> </span>module<span class="w"> </span>spider<span class="w"> </span>octopus/12.1
</pre></div>
</div>
<p>We can see that we have to first load <code class="docutils literal notranslate"><span class="pre">gcc/10.3.0</span></code> and
<code class="docutils literal notranslate"><span class="pre">openmpi/4.1.1</span></code> in order to be able to load and use <code class="docutils literal notranslate"><span class="pre">octopus/12.1</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Sometimes <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span></code> will suggest to either only load a compiler or
compiler + MPI implementation. Then, we generally want to also load the MPI
implementation as only this version of the program will use MPI. Loading the MPI-enabled
version of the desired program is crucial when running a slurm job on multiple nodes.</p>
</div>
<p>We load <code class="docutils literal notranslate"><span class="pre">gcc/10.3.0</span></code>, <code class="docutils literal notranslate"><span class="pre">openmpi/4.1.1</span></code> and finally <code class="docutils literal notranslate"><span class="pre">octopus/12.1</span></code>.
All of this can be done in one line as long as the packages are given in
the correct order (as shown by <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span></code>):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>~$<span class="w"> </span>module<span class="w"> </span>load<span class="w"> </span>gcc/10.3.0<span class="w"> </span>openmpi/4.1.1<span class="w"> </span>octopus/12.1
</pre></div>
</div>
<p>As a first simple check we display the version number of <code class="docutils literal notranslate"><span class="pre">octopus</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>~$ octopus --version
octopus 12.1 (git commit )
</pre></div>
</div>
</section>
<section id="python">
<h3><a class="toc-backref" href="#id15" role="doc-backlink"><span class="section-number">2.4.4. </span>Python</a><a class="headerlink" href="#python" title="Permalink to this heading">#</a></h3>
<p>To use Python we load the <code class="docutils literal notranslate"><span class="pre">anaconda3</span></code> module:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>~$<span class="w"> </span>module<span class="w"> </span>load<span class="w"> </span>anaconda3/2022.05
</pre></div>
</div>
<p>Anaconda comes with a wide variety of pre-installed Python packages such
as <code class="docutils literal notranslate"><span class="pre">numpy</span></code>, <code class="docutils literal notranslate"><span class="pre">scipy</span></code>, <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code>, etc.</p>
<ol class="arabic">
<li><p>Numpy example</p>
<p>We can execute a small demo program called <code class="docutils literal notranslate"><span class="pre">hello-numpy.py</span></code>. The
file has the following content.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hello world&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;numpy version: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>~$ python3 hello_numpy.py
Hello world
numpy version: 1.21.5
[ 0  1  4  9 16]
</pre></div>
</div>
</li>
<li><p>Custom conda environment</p>
<p>We can also create a separate conda environment if we need additional
Python software or different versions. We suggest to use <em>Miniconda</em>
for this (as documented
<a class="reference external" href="https://conda.io/projects/conda/en/latest/user-guide/install/linux.html">here</a>
in more detail).</p>
<p>First, we have to download the Miniconda installer from
<a class="reference external" href="https://docs.conda.io/en/latest/miniconda.html#linux-installers">https://docs.conda.io/en/latest/miniconda.html#linux-installers</a>.
Then, we can run the installer and follow the instructions.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>~$<span class="w"> </span>wget<span class="w"> </span>https://repo.anaconda.com/miniconda/Miniconda3-py310_22.11.1-1-Linux-x86_64.sh
~$<span class="w"> </span>bash<span class="w"> </span>Miniconda3-py310_22.11.1-1-Linux-x86_64.sh
</pre></div>
</div>
<div class="hint admonition">
<p class="admonition-title">Recommendations</p>
<ul>
<li><p>run <code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">init</span></code> when asked about to get access to the conda
executable and</p></li>
<li><p>remove the auto-activation of the base environment to avoid potential conflicts:
<code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">config</span> <span class="pre">--set</span> <span class="pre">auto_activate_base</span> <span class="pre">false</span></code></p>
<p>(If you need to activate the base environment, use <code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">activate</span> <span class="pre">base</span></code>.)</p>
</li>
</ul>
</div>
<p>As an example we now create a new environment, called
<code class="docutils literal notranslate"><span class="pre">my_conda_env</span></code>, with an older version of Python and a specific
numpy version from the <code class="docutils literal notranslate"><span class="pre">conda-forge</span></code> channel.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>~$ conda create -n my_conda_env -c conda-forge python=3.9 numpy=1.23
</pre></div>
</div>
<p>We can now activate the environment and check the versions of Python
and numpy.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>~$ conda activate my_conda_env
~$ python --version
Python 3.9.16
~$ python -c &quot;import numpy; print(numpy.__version__)&quot;
1.23.5
</pre></div>
</div>
<p>We can deactivate and remove the environment using:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>~$ conda deactivate my_conda_env
~$ conda env remove -n my_conda_env
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The local conda executable becomes unusable if you load and unload the
<code class="docutils literal notranslate"><span class="pre">anaconda3</span></code> module. If running into this problem start a new shell.</p>
</div>
</li>
</ol>
</section>
<section id="jupyter-notebooks">
<h3><a class="toc-backref" href="#id16" role="doc-backlink"><span class="section-number">2.4.5. </span>Jupyter notebooks</a><a class="headerlink" href="#jupyter-notebooks" title="Permalink to this heading">#</a></h3>
<p>You can use a Jupyter notebook on a dedicated HPC node as follows:</p>
<ol class="arabic">
<li><p>Ensure you are at MPSD or have the DESY VPN set up.</p></li>
<li><p>Login to a login node (for example <code class="docutils literal notranslate"><span class="pre">mosh</span> <span class="pre">mpsd-hpc-login1.desy.de</span></code>,
<a class="reference external" href="https://mosh.org/">mosh</a> is recommended over <code class="docutils literal notranslate"><span class="pre">ssh</span></code> to avoid
losing the session in case of short connection interruptions e.g. on
WiFi)</p></li>
<li><p>Request a node for interactive use. For example, 1 node for 600
minutes from the <code class="docutils literal notranslate"><span class="pre">public</span></code> partition:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>salloc<span class="w"> </span>--nodes<span class="o">=</span><span class="m">1</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">1</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">1</span><span class="w"> </span>--time<span class="o">=</span><span class="m">06</span>:00:00<span class="w"> </span>-p<span class="w"> </span>public
</pre></div>
</div>
</li>
<li><p>You can install Jupyter yourself, or you activate an installed
version with the following commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>anaconda3
</pre></div>
</div>
</li>
<li><p>Limit numpy (and other libraries) to the available cores</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="nv">$SLURM_CPUS_PER_TASK</span>
</pre></div>
</div>
</li>
<li><p>Start the Jupyter notebook (or Jupyter lab) server on that node with</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>jupyter-notebook<span class="w"> </span>--no-browser<span class="w"> </span>--ip<span class="o">=</span><span class="si">${</span><span class="nv">HOSTNAME</span><span class="si">}</span>.desy.de
</pre></div>
</div>
<p>Watch the output displayed in your terminal. There is a line similar
to this one:</p>
<p><code class="docutils literal notranslate"><span class="pre">http://mpsd-hpc-ibm-055.desy.de:8888/?token=8814fea339b8fe7d3a52e7d03c2ce942a3f35c8c263ff0b8</span></code></p>
<p>which you can paste as a URL into your browser (on your
laptop/Desktop), and you should be connected to the Notebook server
on the compute node.</p>
</li>
</ol>
</section>
<section id="matlab">
<h3><a class="toc-backref" href="#id17" role="doc-backlink"><span class="section-number">2.4.6. </span>Matlab</a><a class="headerlink" href="#matlab" title="Permalink to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">matlab</span></code></p>
</section>
<section id="loading-a-toolchain-to-compile-octopus">
<h3><a class="toc-backref" href="#id18" role="doc-backlink"><span class="section-number">2.4.7. </span>Loading a toolchain to compile octopus</a><a class="headerlink" href="#loading-a-toolchain-to-compile-octopus" title="Permalink to this heading">#</a></h3>
<p>There is also a set of special meta-modules, called <em>toolchains</em>. These
load groups of modules (e.g. compiler, MPI, blas, …). The versions of
the individual packages follow the <a class="reference external" href="https://docs.easybuild.io/en/latest/Common-toolchains.html">easybuild
toolchains</a>.
In addition to the set of modules defined by easybuild the toolchains on
the HPC system also contain all modules required to compile <code class="docutils literal notranslate"><span class="pre">octopus</span></code>
from source.</p>
<p>Here, we show two examples how to compile <code class="docutils literal notranslate"><span class="pre">octopus</span></code>, a serial and an
MPI version. Following this guide is only recommended if you need to
compile octopus from source. We also provide pre-compiled modules for
octopus as outlined in <a class="reference internal" href="#loading-specific-packages"><span class="std std-ref">Loading specific packages</span></a> above.</p>
<p>As mentioned before, different variants of (most) modules are available
that support different CPU feature sets. So far we only discussed the
<em>generic</em> set that can be used on all nodes. In order to make use of all
available features on a specific node we can instead load a more
optimised set of modules. The CPU architecture is available in the
environment variable <code class="docutils literal notranslate"><span class="pre">$MPSD_MICORARCH</span></code>.</p>
<p>First, we remove the <em>generic</em> module set and activate the optimised set
for the current node.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>~$<span class="w"> </span>module<span class="w"> </span>purge
~$<span class="w"> </span>mpsd-modules<span class="w"> </span>-m<span class="w"> </span><span class="nv">$MPSD_MICROARCH</span><span class="w"> </span>dev-23a
</pre></div>
</div>
<ol class="arabic">
<li><p>Parallel version of octopus</p>
<p>We can load the toolchain <code class="docutils literal notranslate"><span class="pre">foss2021a-mpi</span></code> to compile <code class="docutils literal notranslate"><span class="pre">octopus</span></code>
using <code class="docutils literal notranslate"><span class="pre">gcc</span> <span class="pre">10.3.0</span></code> and <code class="docutils literal notranslate"><span class="pre">openmpi</span> <span class="pre">4.1.1</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>~$<span class="w"> </span>module<span class="w"> </span>load<span class="w"> </span>toolchains/foss2021a-mpi
</pre></div>
</div>
<p>Next, we clone octopus:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>~$<span class="w"> </span>git<span class="w"> </span>clone<span class="w"> </span>https://gitlab.com/octopus-code/octopus.git
~$<span class="w"> </span><span class="nb">cd</span><span class="w"> </span>octopus
</pre></div>
</div>
<p>The SSU Computational science maintains a set of configure scripts
that can be used to compile octopus with standard feature sets. These
scripts are available at
<code class="docutils literal notranslate"><span class="pre">/opt_mpsd/linux-debian11/dev-23a/spack-environments/octopus/</span></code>. We
use the following script (<code class="docutils literal notranslate"><span class="pre">foss2021a-mpi-config.sh</span></code>) for this
compilation:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">FCFLAGS</span><span class="o">=</span><span class="s2">&quot;-O2 -g -ffree-line-length-none -fallow-argument-mismatch -fallow-invalid-boz&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">FFLAGS</span><span class="o">=</span><span class="s2">&quot;-O2 -g -ffree-line-length-none -fallow-argument-mismatch -fallow-invalid-boz&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CFLAGS</span><span class="o">=</span><span class="s2">&quot;-O2 -g&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CC</span><span class="o">=</span><span class="s2">&quot;mpicc&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">FC</span><span class="o">=</span><span class="s2">&quot;mpif90&quot;</span>

<span class="c1">#HG: ugly hack to include rpath while linking</span>
<span class="c1">#    becomes necessary for spack &gt;= 0.19, as it does not set LD_LIBRARY_PATH anymore</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LDFLAGS</span><span class="o">=</span><span class="sb">`</span><span class="nb">echo</span><span class="w"> </span><span class="si">${</span><span class="nv">LIBRARY_PATH</span><span class="p">:+:</span><span class="nv">$LIBRARY_PATH</span><span class="si">}</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>sed<span class="w"> </span>-e<span class="w"> </span><span class="s1">&#39;s/:/ -Wl,-rpath=/g&#39;</span><span class="sb">`</span>

../configure<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-libxc-prefix<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$MPSD_LIBXC_ROOT</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-gsl-prefix<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$MPSD_GSL_ROOT</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-sparskit<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$MPSD_SPARSKIT_ROOT</span><span class="s2">/lib/libskit.a&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-nlopt-prefix<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$MPSD_NLOPT_ROOT</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-fftw-prefix<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$MPSD_FFTW_ROOT</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-nfft<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$MPSD_NFFT_ROOT</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-libvdwxc-prefix<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$MPSD_LIBVDWXC_ROOT</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--enable-mpi<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--enable-openmp<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-blas<span class="o">=</span><span class="s2">&quot;-L</span><span class="nv">$MPSD_FLEXIBLAS_ROOT</span><span class="s2">/lib -lflexiblas -lgfortran&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-elpa-prefix<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$MPSD_ELPA_ROOT</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-pfft-prefix<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$MPSD_PFFT_ROOT</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-pnfft-prefix<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$MPSD_PNFFT_ROOT</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-blacs<span class="o">=</span><span class="s2">&quot;-L</span><span class="nv">$MPSD_SCALAPACK_ROOT</span><span class="s2">/lib -lscalapack -lflexiblas -lgfortran&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-berkeleygw-prefix<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$MPSD_BERKELEYGW_ROOT</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="s2">&quot;</span><span class="nv">$@</span><span class="s2">&quot;</span>
</pre></div>
</div>
<p>Now, we can configure and compile octopus:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>~$<span class="w"> </span>autoreconf<span class="w"> </span>-i
~$<span class="w"> </span>mkdir<span class="w"> </span>_build<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>_build
~$<span class="w"> </span>cp<span class="w"> </span>PATH_TO_OCTOPUS_CONFIGURE_SCRIPTS/foss2021a-mpi-config.sh<span class="w"> </span>.
~$<span class="w"> </span><span class="nb">source</span><span class="w"> </span>foss2021a-mpi-config.sh
~$<span class="w"> </span>make
~$<span class="w"> </span>make<span class="w"> </span>check-short
</pre></div>
</div>
</li>
<li><p>Serial version of octopus</p>
<p>Compiling the serial version in principle consists of the same steps
as the parallel version. We use a different toolchain and
configuration script.</p>
<p>We load the serial toolchain:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>~$<span class="w"> </span>module<span class="w"> </span>load<span class="w"> </span>toolchains/foss2021a-serial
</pre></div>
</div>
<p>and use the following configure script
(<code class="docutils literal notranslate"><span class="pre">foss2021a-serial-config.sh</span></code>):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">FCFLAGS</span><span class="o">=</span><span class="s2">&quot;-O2 -g -ffree-line-length-none -fallow-argument-mismatch -fallow-invalid-boz&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">FFLAGS</span><span class="o">=</span><span class="s2">&quot;-O2 -g -ffree-line-length-none -fallow-argument-mismatch -fallow-invalid-boz&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CFLAGS</span><span class="o">=</span><span class="s2">&quot;-O2 -g&quot;</span>

<span class="c1">#HG: ugly hack to include rpath while linking</span>
<span class="c1">#    becomes necessary for spack &gt;= 0.19, as it does not set LD_LIBRARY_PATH anymore</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LDFLAGS</span><span class="o">=</span><span class="sb">`</span><span class="nb">echo</span><span class="w"> </span><span class="si">${</span><span class="nv">LIBRARY_PATH</span><span class="p">:+:</span><span class="nv">$LIBRARY_PATH</span><span class="si">}</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>sed<span class="w"> </span>-e<span class="w"> </span><span class="s1">&#39;s/:/ -Wl,-rpath=/g&#39;</span><span class="sb">`</span>

../configure<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-libxc-prefix<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$MPSD_LIBXC_ROOT</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-gsl-prefix<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$MPSD_GSL_ROOT</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-sparskit<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$MPSD_SPARSKIT_ROOT</span><span class="s2">/lib/libskit.a&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-nlopt-prefix<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$MPSD_NLOPT_ROOT</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-berkeleygw-prefix<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$MPSD_BERKELEYGW_ROOT</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-fftw-prefix<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$MPSD_FFTW_ROOT</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-nfft<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$MPSD_NFFT_ROOT</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-libvdwxc-prefix<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$MPSD_LIBVDWXC_ROOT</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--enable-openmp<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with-blas<span class="o">=</span><span class="s2">&quot;-L</span><span class="nv">$MPSD_FLEXIBLAS_ROOT</span><span class="s2">/lib -lflexiblas -lgfortran&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="s2">&quot;</span><span class="nv">$@</span><span class="s2">&quot;</span>
</pre></div>
</div>
<p>We can then follow the instructions from the previous section.</p>
</li>
</ol>
</section>
<section id="compiling-custom-code">
<h3><a class="toc-backref" href="#id19" role="doc-backlink"><span class="section-number">2.4.8. </span>Compiling custom code</a><a class="headerlink" href="#compiling-custom-code" title="Permalink to this heading">#</a></h3>
<p>To compile other custom code we generally require a different collection
of modules than the one provided by the <code class="docutils literal notranslate"><span class="pre">toolchains</span></code> meta-modules. In
these cases it might be necessary to manually load all required modules.
The same general notes about <em>generic</em> and optimised module sets
explained in the previous section apply.</p>
<p>Here, we show two different examples. The sources are available under
<code class="docutils literal notranslate"><span class="pre">/opt_mpsd/examples/slurm-examples</span></code> (TODO: not available at this
location yet, &lt;2023-02-20 Mon&gt;)</p>
<ol class="arabic">
<li><p>Serial “Hello world” in Fortran</p>
<p>First, we want to compile the following “hello world” Fortran program
using <code class="docutils literal notranslate"><span class="pre">gcc</span></code>. We assume it is saved in a file <code class="docutils literal notranslate"><span class="pre">hello.f90</span></code>. The
source is available in
<code class="docutils literal notranslate"><span class="pre">/opt_mpsd/examples/slurm-examples/serial-fortran</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>program<span class="w"> </span>hello
<span class="w">  </span>write<span class="o">(</span>*,*<span class="o">)</span><span class="w"> </span><span class="s2">&quot;Hello world!&quot;</span>
end<span class="w"> </span>program
</pre></div>
</div>
<p>We have to load <code class="docutils literal notranslate"><span class="pre">gcc</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>gcc/10.3.0
</pre></div>
</div>
<p>Then, we can compile and execute the program:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>~$<span class="w"> </span>gfortran<span class="w"> </span>-o<span class="w"> </span>hello<span class="w"> </span>hello.f90
~$<span class="w"> </span>./hello
<span class="w"> </span>Hello<span class="w"> </span>world!
</pre></div>
</div>
</li>
<li><p>MPI-parallelised “Hello world” in C</p>
<p>As a second example we compile an MPI-parallelised “Hello world” C
program, again using <code class="docutils literal notranslate"><span class="pre">gcc</span></code>. We assume the source is saved in a file
<code class="docutils literal notranslate"><span class="pre">hello-mpi.c</span></code> (source available under
<code class="docutils literal notranslate"><span class="pre">/opt_mpsd/examples/slurm-examples/mpi-c</span></code>).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#include &lt;mpi.h&gt;</span>
<span class="c1">#include &lt;stdio.h&gt;</span>

int<span class="w"> </span>main<span class="o">(</span>int<span class="w"> </span>argc,<span class="w"> </span>char**<span class="w"> </span>argv<span class="o">)</span><span class="w"> </span><span class="o">{</span>
<span class="w">    </span>MPI_Init<span class="o">(</span>NULL,<span class="w"> </span>NULL<span class="o">)</span><span class="p">;</span>

<span class="w">    </span>int<span class="w"> </span>world_size<span class="p">;</span>
<span class="w">    </span>MPI_Comm_size<span class="o">(</span>MPI_COMM_WORLD,<span class="w"> </span><span class="p">&amp;</span>world_size<span class="o">)</span><span class="p">;</span>

<span class="w">    </span>int<span class="w"> </span>world_rank<span class="p">;</span>
<span class="w">    </span>MPI_Comm_rank<span class="o">(</span>MPI_COMM_WORLD,<span class="w"> </span><span class="p">&amp;</span>world_rank<span class="o">)</span><span class="p">;</span>

<span class="w">    </span>char<span class="w"> </span>processor_name<span class="o">[</span>MPI_MAX_PROCESSOR_NAME<span class="o">]</span><span class="p">;</span>
<span class="w">    </span>int<span class="w"> </span>name_len<span class="p">;</span>
<span class="w">    </span>MPI_Get_processor_name<span class="o">(</span>processor_name,<span class="w"> </span><span class="p">&amp;</span>name_len<span class="o">)</span><span class="p">;</span>

<span class="w">    </span>printf<span class="o">(</span><span class="s2">&quot;Hello world from rank %d out of %d on %s.\n&quot;</span>,
<span class="w">           </span>world_rank,<span class="w"> </span>world_size,<span class="w"> </span>processor_name<span class="o">)</span><span class="p">;</span>

<span class="w">    </span>MPI_Finalize<span class="o">()</span><span class="p">;</span>
<span class="o">}</span>
</pre></div>
</div>
<p>We have to load <code class="docutils literal notranslate"><span class="pre">gcc</span></code> and <code class="docutils literal notranslate"><span class="pre">openmpi</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>~$<span class="w"> </span>module<span class="w"> </span>load<span class="w"> </span>gcc/10.3.0<span class="w"> </span>openmpi/4.1.1
</pre></div>
</div>
<p>Now, we can compile and execute the test program:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>~$ mpicc -o hello-mpi hello-mpi.c
~$ orterun -n 4 ./hello-mpi
Hello world from rank 2 out of 4 on mpsd-hpc-login1.
Hello world from rank 3 out of 4 on mpsd-hpc-login1.
Hello world from rank 1 out of 4 on mpsd-hpc-login1.
Hello world from rank 0 out of 4 on mpsd-hpc-login1.
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Inside a slurm job <code class="docutils literal notranslate"><span class="pre">srun</span></code> has to be used instead of <code class="docutils literal notranslate"><span class="pre">orterun</span></code>.</p>
</div>
</li>
</ol>
</section>
</section>
<section id="example-batch-scripts">
<span id="examplebatchscripts"></span><h2><a class="toc-backref" href="#id20" role="doc-backlink"><span class="section-number">2.5. </span>Example batch scripts</a><a class="headerlink" href="#example-batch-scripts" title="Permalink to this heading">#</a></h2>
<p>Here, we show a number of example batch scripts for different types of
jobs. All examples are available on the HPC system under
<code class="docutils literal notranslate"><span class="pre">/opt_mpsd/examples/slurm-examples</span></code> together with the example
programs. (TODO: not available yet, &lt;2023-02-20 Mon&gt;) We use the
<code class="docutils literal notranslate"><span class="pre">mpsd</span></code> partition and the <em>generic</em> module set for all examples.</p>
<p>To test an example on the HPC system we can copy the relevant directory
into our scratch directory. If required we can compile the code using
<code class="docutils literal notranslate"><span class="pre">make</span></code> and then submit the job using <code class="docutils literal notranslate"><span class="pre">sbatch</span> <span class="pre">submission-script.sh</span></code>.</p>
<section id="mpi">
<h3><a class="toc-backref" href="#id21" role="doc-backlink"><span class="section-number">2.5.1. </span>MPI</a><a class="headerlink" href="#mpi" title="Permalink to this heading">#</a></h3>
<p>The source code and submission script are in
<code class="docutils literal notranslate"><span class="pre">/opt_mpsd/examples/slurm-examples/mpi-c</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash --login</span>
<span class="c1">#</span>
<span class="c1"># Standard output and error</span>
<span class="c1">#SBATCH -o ./out.%j</span>
<span class="c1">#SBATCH -e ./err.%j</span>
<span class="c1">#</span>
<span class="c1"># working directory</span>
<span class="c1">#SBATCH -D ./</span>
<span class="c1">#</span>
<span class="c1"># partition</span>
<span class="c1">#SBATCH -p public</span>
<span class="c1">#</span>
<span class="c1"># job name</span>
<span class="c1">#SBATCH -J MPI-example</span>
<span class="c1">#</span>
<span class="c1">#SBATCH --mail-type=ALL</span>
<span class="c1">#</span>
<span class="c1"># job requirements</span>
<span class="c1">#SBATCH --nodes=4</span>
<span class="c1">#SBATCH --ntasks-per-node=16</span>
<span class="c1">#SBATCH --time=00:10:00</span>

.<span class="w"> </span>setup-env.sh

srun<span class="w"> </span>./hello-mpi
</pre></div>
</div>
</section>
<section id="mpi-openmp">
<span id="example-openmp-mpi"></span><h3><a class="toc-backref" href="#id22" role="doc-backlink"><span class="section-number">2.5.2. </span>MPI + OpenMP</a><a class="headerlink" href="#mpi-openmp" title="Permalink to this heading">#</a></h3>
<p>The source code and submission script are in
<code class="docutils literal notranslate"><span class="pre">/opt_mpsd/examples/slurm-examples/mpi-openmp-c</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash --login</span>
<span class="c1">#</span>
<span class="c1"># Standard output and error</span>
<span class="c1">#SBATCH -o ./out.%j</span>
<span class="c1">#SBATCH -e ./err.%j</span>
<span class="c1">#</span>
<span class="c1"># working directory</span>
<span class="c1">#SBATCH -D ./</span>
<span class="c1">#</span>
<span class="c1"># partition</span>
<span class="c1">#SBATCH -p public</span>
<span class="c1">#</span>
<span class="c1"># job name</span>
<span class="c1">#SBATCH -J MPI-OpenMP-example</span>
<span class="c1">#</span>
<span class="c1">#SBATCH --mail-type=ALL</span>
<span class="c1">#</span>
<span class="c1"># job requirements</span>
<span class="c1">#SBATCH --nodes=4</span>
<span class="c1">#SBATCH --ntasks-per-node=2</span>
<span class="c1">#SBATCH --cpus-per-task=8</span>
<span class="c1">#SBATCH --time=00:10:00</span>

.<span class="w"> </span>setup-env.sh

<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="si">}</span>

<span class="c1"># TODO from the MPCDF example, does the same apply here?</span>
<span class="c1"># For pinning threads correctly:</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_PLACES</span><span class="o">=</span>cores

srun<span class="w"> </span>./hello-mpi-openmp
</pre></div>
</div>
</section>
<section id="openmp">
<span id="example-openmp"></span><h3><a class="toc-backref" href="#id23" role="doc-backlink"><span class="section-number">2.5.3. </span>OpenMP</a><a class="headerlink" href="#openmp" title="Permalink to this heading">#</a></h3>
<p>The source code and submission script are in
<code class="docutils literal notranslate"><span class="pre">/opt_mpsd/examples/slurm-examples/openmp-c</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash --login</span>
<span class="c1">#</span>
<span class="c1"># Standard output and error</span>
<span class="c1">#SBATCH -o ./out.%j</span>
<span class="c1">#SBATCH -e ./err.%j</span>
<span class="c1">#</span>
<span class="c1"># working directory</span>
<span class="c1">#SBATCH -D ./</span>
<span class="c1">#</span>
<span class="c1"># partition</span>
<span class="c1">#SBATCH -p public</span>
<span class="c1">#</span>
<span class="c1"># job name</span>
<span class="c1">#SBATCH -J OpenMP-example</span>
<span class="c1">#</span>
<span class="c1">#SBATCH --mail-type=ALL</span>
<span class="c1">#</span>
<span class="c1"># job requirements</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --ntasks-per-node=1</span>
<span class="c1">#SBATCH --cpus-per-task=16</span>
<span class="c1">#SBATCH --time=00:10:00</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="si">}</span>

<span class="c1"># TODO from the MPCDF example, does the same apply here?</span>
<span class="c1"># For pinning threads correctly:</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_PLACES</span><span class="o">=</span>cores

srun<span class="w"> </span>./hello-openmp
</pre></div>
</div>
</section>
<section id="python-with-numpy-or-multiprocessing">
<span id="example-python-multiprocessing"></span><h3><a class="toc-backref" href="#id24" role="doc-backlink"><span class="section-number">2.5.4. </span>Python with numpy or multiprocessing</a><a class="headerlink" href="#python-with-numpy-or-multiprocessing" title="Permalink to this heading">#</a></h3>
<p>The source code and submission script are in
<code class="docutils literal notranslate"><span class="pre">/opt_mpsd/examples/slurm-examples/python-numpy</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash --login</span>
<span class="c1">#</span>
<span class="c1"># Standard output and error</span>
<span class="c1">#SBATCH -o ./out.%j</span>
<span class="c1">#SBATCH -e ./err.%j</span>
<span class="c1">#</span>
<span class="c1"># working directory</span>
<span class="c1">#SBATCH -D ./</span>
<span class="c1">#</span>
<span class="c1"># partition</span>
<span class="c1">#SBATCH -p public</span>
<span class="c1">#</span>
<span class="c1"># job name</span>
<span class="c1">#SBATCH -J python-numpy-example</span>
<span class="c1">#</span>
<span class="c1">#SBATCH --mail-type=ALL</span>
<span class="c1">#</span>
<span class="c1"># job requirements</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --ntasks-per-node=1</span>
<span class="c1">#SBATCH --cpus-per-task=16</span>
<span class="c1">#SBATCH --time=00:10:00</span>

module<span class="w"> </span>purge
mpsd-modules<span class="w"> </span>dev-23a

module<span class="w"> </span>load<span class="w"> </span>anaconda/2022.05

<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="si">}</span>

srun<span class="w"> </span>python3<span class="w"> </span>./hello-numpy.py
</pre></div>
</div>
</section>
<section id="single-core-job">
<span id="example-singlecore"></span><h3><a class="toc-backref" href="#id25" role="doc-backlink"><span class="section-number">2.5.5. </span>Single-core job</a><a class="headerlink" href="#single-core-job" title="Permalink to this heading">#</a></h3>
<p>The source code and submission script are in
<code class="docutils literal notranslate"><span class="pre">/opt_mpsd/examples/slurm-examples/serial-fortran</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash --login</span>
<span class="c1">#</span>
<span class="c1"># Standard output and error</span>
<span class="c1">#SBATCH -o ./out.%j</span>
<span class="c1">#SBATCH -e ./err.%j</span>
<span class="c1">#</span>
<span class="c1"># working directory</span>
<span class="c1">#SBATCH -D ./</span>
<span class="c1">#</span>
<span class="c1"># partition</span>
<span class="c1">#SBATCH -p public</span>
<span class="c1">#</span>
<span class="c1"># job name</span>
<span class="c1">#SBATCH -J serial-example</span>
<span class="c1">#</span>
<span class="c1">#SBATCH --mail-type=ALL</span>
<span class="c1">#</span>
<span class="c1"># job requirements</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --ntasks-per-node=1</span>
<span class="c1">#SBATCH --time=00:10:00</span>

<span class="c1"># TODO do we want to use any modules for this example</span>

srun<span class="w"> </span>./hello
</pre></div>
</div>
</section>
<section id="serial-python">
<h3><a class="toc-backref" href="#id26" role="doc-backlink"><span class="section-number">2.5.6. </span>Serial Python</a><a class="headerlink" href="#serial-python" title="Permalink to this heading">#</a></h3>
<p>The source code and submission script are in
<code class="docutils literal notranslate"><span class="pre">/opt_mpsd/examples/slurm-examples/python-serial</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash --login</span>
<span class="c1">#</span>
<span class="c1"># Standard output and error</span>
<span class="c1">#SBATCH -o ./out.%j</span>
<span class="c1">#SBATCH -e ./err.%j</span>
<span class="c1">#</span>
<span class="c1"># working directory</span>
<span class="c1">#SBATCH -D ./</span>
<span class="c1">#</span>
<span class="c1"># partition</span>
<span class="c1">#SBATCH -p public</span>
<span class="c1">#</span>
<span class="c1"># job name</span>
<span class="c1">#SBATCH -J python-example</span>
<span class="c1">#</span>
<span class="c1">#SBATCH --mail-type=ALL</span>
<span class="c1">#</span>
<span class="c1"># job requirements</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --ntasks-per-node=1</span>
<span class="c1">#SBATCH --time=00:10:00</span>

module<span class="w"> </span>purge
mpsd-modules<span class="w"> </span>dev-23a

module<span class="w"> </span>load<span class="w"> </span>anaconda/2022.05

<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span><span class="w">  </span><span class="c1"># restrict numpy (and other libraries) to one core</span>

srun<span class="w"> </span>python3<span class="w"> </span>./hello.py
</pre></div>
</div>
</section>
<section id="gpu-jobs">
<h3><a class="toc-backref" href="#id27" role="doc-backlink"><span class="section-number">2.5.7. </span>GPU jobs</a><a class="headerlink" href="#gpu-jobs" title="Permalink to this heading">#</a></h3>
<p>For GPU jobs, we recommend to specify desired hardware resources as
follows. In parenthesis we provide the typical application (MPI, OpenMP)
for guidance.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">nodes</span></code> - how many computers to use, for example <code class="docutils literal notranslate"><span class="pre">--nodes=1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tasks-per-node</span></code> - how many (MPI) processed to run per node:
<code class="docutils literal notranslate"><span class="pre">--tasks-per-node=4</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpus-per-task</span></code> - how many GPUs per (MPI) process to use (often 1):
<code class="docutils literal notranslate"><span class="pre">--gpus-per-task=1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cpus-per-task</span></code> - how many CPUs (OpenMP threads) to use:
<code class="docutils literal notranslate"><span class="pre">--cpus-per-task=4</span></code></p></li>
</ul>
<p>Example:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>user@mpsd-hpc-login1:~$<span class="w"> </span>salloc<span class="w"> </span>--nodes<span class="o">=</span><span class="m">1</span><span class="w"> </span>--tasks-per-node<span class="o">=</span><span class="m">4</span><span class="w"> </span>--gpus-per-task<span class="o">=</span><span class="m">1</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">4</span><span class="w"> </span>--mem<span class="o">=</span>128G<span class="w"> </span>-p<span class="w"> </span>gpu
user@mpsd-hpc-gpu-002:~$<span class="w"> </span>mpsd-show-job-resources
<span class="w">   </span><span class="m">9352</span><span class="w"> </span>Nodes:<span class="w"> </span>mpsd-hpc-gpu-002
<span class="w">   </span><span class="m">9352</span><span class="w"> </span>Local<span class="w"> </span>Node:<span class="w"> </span>mpsd-hpc-gpu-002

<span class="w">   </span><span class="m">9352</span><span class="w"> </span>CPUSET:<span class="w"> </span><span class="m">0</span>-7,16-23
<span class="w">   </span><span class="m">9352</span><span class="w"> </span>MEMORY:<span class="w"> </span><span class="m">131072</span><span class="w"> </span>M

<span class="w">   </span><span class="m">9352</span><span class="w"> </span>GPUs<span class="w"> </span><span class="o">(</span>Interconnects,<span class="w"> </span>CPU<span class="w"> </span>Affinity,<span class="w"> </span>NUMA<span class="w"> </span>Affinity<span class="o">)</span>:
<span class="w">   </span><span class="m">9352</span><span class="w"> </span>GPU0<span class="w">     </span>X<span class="w">  </span>NV1<span class="w"> </span>NV1<span class="w"> </span>NV2<span class="w"> </span>SYS<span class="w"> </span><span class="m">0</span>-7,16-23<span class="w">   </span><span class="m">0</span>-1
<span class="w">   </span><span class="m">9352</span><span class="w"> </span>GPU1<span class="w">    </span>NV1<span class="w">  </span>X<span class="w">  </span>NV2<span class="w"> </span>NV1<span class="w"> </span>SYS<span class="w"> </span><span class="m">0</span>-7,16-23<span class="w">   </span><span class="m">0</span>-1
<span class="w">   </span><span class="m">9352</span><span class="w"> </span>GPU2<span class="w">    </span>NV1<span class="w"> </span>NV2<span class="w">  </span>X<span class="w">  </span>NV2<span class="w"> </span>SYS<span class="w"> </span><span class="m">0</span>-7,16-23<span class="w">   </span><span class="m">0</span>-1
<span class="w">   </span><span class="m">9352</span><span class="w"> </span>GPU3<span class="w">    </span>NV2<span class="w"> </span>NV1<span class="w"> </span>NV2<span class="w">  </span>X<span class="w">  </span>SYS<span class="w"> </span><span class="m">0</span>-7,16-23<span class="w">   </span><span class="m">0</span>-1
user@mpsd-hpc-gpu-002:~$
</pre></div>
</div>
<p>We can see from the output that we have one node (mpsd-hpc-gpu-002), 16
CPUs (with ids 0 to 7 and 16 to 23), 128GB (=131072MiB), and 4 GPUs
allocated (GPU0 to GPU3).</p>
<p>(TODO: Add CUDA Slurm example program and submission script.)</p>
</section>
</section>
</section>


            </article>
            
            
            
            <footer class="bd-footer-article">
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
  <a class='left-prev' id="prev-link" href="overview-it.html" title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
          <p class="prev-next-subtitle">previous</p>
          <p class="prev-next-title"><span class="section-number">1. </span>Overview computing services</p>
      </div>
  </a>
  <a class='right-next' id="next-link" href="software.html" title="next page">
  <div class="prev-next-info">
      <p class="prev-next-subtitle">next</p>
      <p class="prev-next-title"><span class="section-number">3. </span>Software</p>
  </div>
  <i class="fa-solid fa-angle-right"></i>
  </a>
</div>
            </footer>
            
          </div>
          
          
          
            <div class="bd-sidebar-secondary bd-toc">
              
<div class="toc-item">
  
<div class="tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
</div>
<nav id="bd-toc-nav" class="page-toc">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#login-nodes">
   2.1. Login nodes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#job-submission">
   2.2. Job submission
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partitions">
     2.2.1. Partitions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#slurm-default-values">
     2.2.2. Slurm default values
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#slurm-cpus">
     2.2.3. Slurm CPUs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interactive-use-of-hpc-nodes">
     2.2.4. Interactive use of HPC nodes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#finding-about-about-my-jobs">
     2.2.5. Finding about about my jobs
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#storage-and-quotas">
   2.3. Storage and quotas
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#software">
   2.4. Software
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tldr">
     2.4.1. TLDR
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initial-setup">
     2.4.2. Initial setup
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loading-specific-packages">
     2.4.3. Loading specific packages
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#python">
     2.4.4. Python
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#jupyter-notebooks">
     2.4.5. Jupyter notebooks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#matlab">
     2.4.6. Matlab
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loading-a-toolchain-to-compile-octopus">
     2.4.7. Loading a toolchain to compile octopus
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compiling-custom-code">
     2.4.8. Compiling custom code
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-batch-scripts">
   2.5. Example batch scripts
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mpi">
     2.5.1. MPI
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mpi-openmp">
     2.5.2. MPI + OpenMP
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#openmp">
     2.5.3. OpenMP
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#python-with-numpy-or-multiprocessing">
     2.5.4. Python with numpy or multiprocessing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#single-core-job">
     2.5.5. Single-core job
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#serial-python">
     2.5.6. Serial Python
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gpu-jobs">
     2.5.7. GPU jobs
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
</div>

<div class="toc-item">
  
<div id="searchbox"></div>
</div>

<div class="toc-item">
  
</div>

<div class="toc-item">
  
</div>

            </div>
          
          
        </div>
        <footer class="bd-footer-content">
          <div class="bd-footer-content__inner">
            
          </div>
        </footer>
        
      </main>
    </div>
  </div>

  
    
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94"></script>

  <footer class="bd-footer"><div class="bd-footer__inner container">
  
  <div class="footer-item">
    
<p class="copyright">

    &copy; Copyright 2021- Hans Fangohr, MPSD.<br>

</p>

  </div>
  
</div>
  </footer>
  </body>
</html>