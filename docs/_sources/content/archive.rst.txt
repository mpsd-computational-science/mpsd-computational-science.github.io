


Archiving research data
-----------------------

As part of :doc:`research-data-management`, we may need to or want to
archive data for longer periods (5 years, 10 years or more).

What data to archive?
~~~~~~~~~~~~~~~~~~~~~

Storage space is not free (or cheap): we should only archive data that could be useful to others or us at some point in the future.

First, archive data that needs to be archived
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For certain research activities, we may have to archive the data. The prime example are research data associated with publications.

It is also possible that research (grant) agreements require particular data sets to be archived beyond the run time of a (funded) project.

Second, consider additional research data sets for archival
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Before we archive a data set which we are not required for reasons such as
contractual agreements or required best practice, we should consider if those
data sets can truly be useful in the future (either to us, or others):

- Is our documentation of the data sufficiently good that we could make sense of it in 2 years time (for example)?

- If we have not managed to analyse the paper now and put it into a manuscript, why do we think we would have more time/capacity to do that later in the future?

- Would somebody not familiar with the project be able to benefit from the data? (In particular: have we explained what the data represents in sufficient details?)

Third, consider if data sets can be given up on
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Important and potentially useful data should be kept, but where it is very
unlikely that (further) scientific advances can be made, deleting the data might
be a reasonable way forward: this will free resources that can be used for other
data sets that may have a higher chance of creating impact.

Long-term storage of data sets - be it for analysis or archival - requires staff
time, electricity, hardware, hardware maintenance, refreshing of old tapes, etc.
This creates cost, the amount of which is not always visible or known to the
researcher.

It is not unusual for an experience to acquire significant amounts of data; out
of which, for example, 20% are used in publications. The question raised in this
section is: should we archive the other 80% just in case they contain useful
data. There is no generic answer to this, but it might be useful to raise the
question openly, and discuss it between scientists and infrastructure (storage)
experts.

What should I do before archiving the data (meta data)?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A significant challenge is *to document* the data.

This includes a description of the format, the meaning of the data, any
assumptions made in the capturing or processing of the data. If software was
used to create the data, or if software is required to read the data, the
software should be included or the very least a reference to the software
repository.

Any information that would be required to (re-)use the data in the future should
be included: it should be possible for others to extract, inspect and use the
data in the future, without having to consult you or your co-workers to request
such information.

This type of information is often called *meta data*. It is required to explain the data.

The use of some :ref:`domain-specific-file-formats` can much simplify
the documentation of data, as the metadata is embedded in the data file
automatically.

Archival options for Max Planck researchers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

`GWDG Archival service <https://info.gwdg.de/dokuwiki/doku.php?id=en:services:storage_services:data_archiving:start>`_: no space limit/order of magnitude mentioned

`Max Planck Digital Library <https://mpdl.zendesk.com/hc/en-us/articles/360011432700-Archiving>`_: data sets up to 500GB (more possible on request), use
of `Keeper <https://keeper.mpdl.mpg.de/>`_ for Archival.

`Max Planck Compute & Data Facility (MPCDF) <https://www.mpcdf.mpg.de/services/data/backup-archive/archives>`_: recommend to use file sizes between 1GB and 1TB

Other archival options
~~~~~~~~~~~~~~~~~~~~~~

`Zenodo <https://zenodo.org>`_ offers archival of small data sets (up to `50GB without special requests <https://help.zenodo.org>`_), and provides a DOI for such submissions.

Related information
~~~~~~~~~~~~~~~~~~~

.. seealso::

    - From HPC-Carpentry: `transfer of data and use of tar/zip <http://rits.github-pages.ucl.ac.uk/hpc-intro/15-transferring-files/index.html>`_
