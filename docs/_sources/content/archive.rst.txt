


Archiving research data
-----------------------

As part of :doc:`research-data-management`, we may need to or want to
archive data for longer periods (5 years, 10 years or more).

What data to archive?
~~~~~~~~~~~~~~~~~~~~~

Storage space is not free (or cheap): we should only archive data that could be useful to others or us at some point in the future.

First, archive data that needs to be archived
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For certain research activities, we may have to archive the data. The prime example are research data associated with publications.

It is also possible that research (grant) agreements require particular data sets to be archived beyond the run time of a (funded) project.

Second, consider additional research data sets for archival
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Before we archive a data set which we are not required for reasons such as
contractual agreements or required best practice, we should consider if those
data sets can truly be useful in the future (either to us, or others):

- Is our documentation of the data sufficiently good that we could make sense of it in 2 years time (for example)?

- If we have not managed to analyse the paper now and put it into a manuscript, why do we think we would have more time/capacity to do that later in the future?

- Would somebody not familiar with the project be able to benefit from the data? (In particular: have we explained what the data represents in sufficient details?)

What should I do before archiving the data?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. important:: [write this section]

Archival options for Max Planck researchers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

`GWDG Archival service <https://info.gwdg.de/dokuwiki/doku.php?id=en:services:storage_services:data_archiving:start>`_: no space limit/order of magnitude mentioned

`Max Planck Digital Library <https://mpdl.zendesk.com/hc/en-us/articles/360011432700-Archiving>`_: data sets up to 500GB (more possible on request), use
of `Keeper <https://keeper.mpdl.mpg.de/>`_ for Archival.

`Max Planck Compute & Data Facility (MPCDF) <https://www.mpcdf.mpg.de/services/data/backup-archive/archives>`_: recommend to use file sizes between 1GB and 1TB

Other archival options
~~~~~~~~~~~~~~~~~~~~~~

`Zenodo <https://zenodo.org>`_ offers archival of small data sets (up to `50GB without special requests <https://help.zenodo.org>`_), and provides a DOI for such submissions.

Related information
~~~~~~~~~~~~~~~~~~~

.. seealso::

    - From HPC-Carpentry: `transfer of data and use of tar/zip <http://rits.github-pages.ucl.ac.uk/hpc-intro/15-transferring-files/index.html>`_
