.. _mpsd_hpc:

MPSD HPC system
===============

.. contents:: :local:

.. important::

   We are working on an upgrade of software stack and documentation on the local
   HPC cluster. The cluster will be unavailable until the end of this maintenance.

For the Raven and Ada machine, please check :doc:`overview-it`.

Login nodes
-----------

Login nodes are ``mpsd-hpc-login1.desy.de`` and
``mpsd-hpc-login2.desy.de``.

.. note::

   During the first login your home directory will be created. This could take up to a minute. Please be patient.

Job submission
--------------

Job submission is via `Slurm <https://slurm.schedmd.com>`__.

Example slurm submission jobs are available below
(:ref:`examplebatchscripts`).

Partitions
~~~~~~~~~~

The following partitions are available to all (partial output from
``sinfo``):

::

   PARTITION  AVAIL  TIMELIMIT  NODES NODELIST
   bigmem        up 7-00:00:00      8 mpsd-hpc-hp-[001-008]
   gpu           up 7-00:00:00      2 mpsd-hpc-gpu-[001-002]
   gpu-ayyer     up 7-00:00:00      3 mpsd-hpc-gpu-[003-005]
   public*       up 7-00:00:00     49 mpsd-hpc-ibm-[001-030,035-036,043-049,053-062]

Please use the machines in the ``gpu`` partition only if your code
supports nvidia-cuda.

Hardware resources per node:

-  ``public`` partitions

   -  8 physical cores (16 with hyperthreading)
   -  64GB RAM
   -  at most 2 nodes can be used for multi-node jobs (as the 10GB
      ethernet for MPI has a relatively high latency)

-  ``bigmem``

   -  96 physical cores (192 with hyperthreading)
   -  2T RAM
   -  fast FDR infiniband for MPI communication

-  ``gpu``

   -  16 physical cores (32 with hyperthreading)
   -  1.5T RAM
   -  fast FDR infiniband for MPI communication
   -  8 Tesla V100 GPUs
   -  at most 2 nodes can be used for multi-node jobs (as the 10GB
      ethernet for MPI has a relatively high latency)

The maximum runtime for any job is 7 days (that is 10080 minutes).

Slurm default values
~~~~~~~~~~~~~~~~~~~~

Slurm defaults are an execution time of 1 hour, one task, one CPU, 3.5GB
of RAM, and the ``public`` partition.

Nodes by default are shared between multiple users (i.e. use is not
exclusive, unless requested).

Logging onto nodes via ``ssh`` is only possible once the nodes are
allocated (either via ``sbatch`` when the job starts, of using
``salloc`` for interactive use, see
:ref:`interactive-use-of-HPC-nodes`). This avoids accidental over-use of
resources and enables energy saving measures (such as switching compute
nodes off automatically if they are not in use).

Slurm CPUs
~~~~~~~~~~

Note that a "CPU" in Slurm terminology is a `computational core (or a
thread if hyperthreading is
configured) <https://slurm.schedmd.com/faq.html#cpu_count>`__, whereas
in the language in daily use we would consider a (populated) CPU
(socket) to be the device holding many computational cores (such as 16
cores per CPU socket or so). We use the Slurm convention in this
document as do the slurm commands.

.. _interactive-use-of-HPC-nodes:

Interactive use of HPC nodes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For production computation, we typically write a batch file (see
:ref:`examplebatchscripts`), and submit these using the ``sbatch``
command.

Sometimes, it can be helpful to login into an HPC node for example to
compile software or run interactive tests. The command to use in this
case is ``salloc``.

For example, requesting a job with all default settings:

.. code:: console

   user@mpsd-hpc-login1:~$ salloc 
   salloc: Granted job allocation 1272
   user@mpsd-hpc-ibm-058:~$

We can see from the prompt (``user@mpsd-hpc-ibm-058:~$``) that the Slurm
system has allocated node ``mpsd-hpc-ibm-058`` to us.

We can use the ``mpsd-show-job-resources`` command to check some details
of the allocation:

.. code:: console

   user@mpsd-hpc-ibm-058:~$ mpsd-show-job-resources 
    345415 Nodes: mpsd-hpc-ibm-058
    345415 Local Node: mpsd-hpc-ibm-058

    345415 CPUSET: 0
    345415 MEMORY: 3500 M

We can finish our interactive session by typing ``exit``.

If we desire exclusive use of a node (i.e. not shared with others), we
can use ``salloc --exclusive`` (he we request a session time of 120
minutes):

.. code:: console

   user@mpsd-hpc-login2:~$ salloc  --exclusive --time=120
   salloc: Granted job allocation 1279
   user@mpsd-hpc-ibm-061:~$ mpsd-show-job-resources 
     65911 Nodes: mpsd-hpc-ibm-061
     65911 Local Node: mpsd-hpc-ibm-061

     65911 CPUSET: 0-15
     65911 MEMORY: 56000 M

We can see (in the output above) that all 16 CPUs of the node are
allocated to us.

Assume we need 16 CPUs and 10GB of RAM for our interactive session (the
16 CPUs corresponds to the number of OpenMP threads, see
:ref:`example-openmp`):

.. code:: console

   user@mpsd-hpc-login1:~$ salloc --mem=10000 --cpus-per-task=16
   salloc: Granted job allocation 1273
   user@mpsd-hpc-ibm-058:~$ mpsd-show-job-resources 
    345446 Nodes: mpsd-hpc-ibm-058
    345446 Local Node: mpsd-hpc-ibm-058

    345446 CPUSET: 0-15
    345446 MEMORY: 10000 M
   user@mpsd-hpc-ibm-058:~

If we execute MPI programs, we need to specify the number of nodes (a
node is a computer node, with typically one, two or four CPU sockets),
and how many (MPI) tasks (=processes) we want to run on that node.
Imagine we ask for two nodes, and want to run 4 MPI processes on each:

.. code:: console

   user@mpsd-hpc-login1:~$ salloc --nodes=2 --tasks-per-node=4
   salloc: Granted job allocation 1276
   user@mpsd-hpc-ibm-058:~$ mpsd-show-job-resources 
    345591 Nodes: mpsd-hpc-ibm-[058-059]
    345591 Local Node: mpsd-hpc-ibm-058

    345591 CPUSET: 0-3
    345591 MEMORY: 14000 M
   user@mpsd-hpc-ibm-058:~$ srun hostname
   mpsd-hpc-ibm-059
   mpsd-hpc-ibm-059
   mpsd-hpc-ibm-059
   mpsd-hpc-ibm-059
   mpsd-hpc-ibm-058
   mpsd-hpc-ibm-058
   mpsd-hpc-ibm-058
   mpsd-hpc-ibm-058

The ``srun`` command starts the execution of our (MPI) tasks. We use the
``hostname`` command above and can see that we have 4 of these commands
run on each node.

Finding about about my jobs
~~~~~~~~~~~~~~~~~~~~~~~~~~~

There are multiple ways of finding out about your slurm jobs:

-  ``squeue --me`` lists only your jobs (see below for output)
-  ``mpsd-show-job-resources`` can be used 'inside' the job (to verify
   hardware allocation is as desired)
-  ``scontrol show job JOBID`` provides a lot of detail

Example: We request 2 nodes, with 4 tasks (and by default one CPU per
task)

::

   fangohr@mpsd-hpc-login1:~$ salloc --nodes=2 --tasks-per-node=4
   salloc: Granted job allocation 1276

   fangohr@mpsd-hpc-login2:~$ squeue --me
     JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
      1276    public interact  fangohr  R      11:37      2 mpsd-hpc-ibm-[058-059]

   fangohr@mpsd-hpc-ibm-058:~$ mpsd-show-job-resources 
    345591 Nodes: mpsd-hpc-ibm-[058-059]
    345591 Local Node: mpsd-hpc-ibm-058

    345591 CPUSET: 0-3
    345591 MEMORY: 14000 M

   fangohr@mpsd-hpc-login2:~$ scontrol show job 1276
   JobId=1276 JobName=interactive
      UserId=fangohr(28479) GroupId=cfel(3512) MCS_label=N/A
      <...>
      RunTime=00:14:08 TimeLimit=01:00:00 TimeMin=N/A
      Partition=public AllocNode:Sid=mpsd-hpc-login1.desy.de:3116660
      NodeList=mpsd-hpc-ibm-[058-059]
      BatchHost=mpsd-hpc-ibm-058
      NumNodes=2 NumCPUs=8 NumTasks=8 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
      TRES=cpu=8,mem=28000M,node=2,billing=8
      Socks/Node=* NtasksPerN:B:S:C=4:0:*:1 CoreSpec=*
      MinCPUsNode=4 MinMemoryCPU=3500M MinTmpDiskNode=0
      <...>

Storage and quotas
------------------

``/home/<username>`` (``$HOME``)

-  home file system for code and scripts
-  user quota (storage limit): 100 GB
-  quota can be checked with ``df -h $HOME``
-  regular backups
-  users have access to the backup of their data under
   ``$HOME/.zfs/snapshots``

``/scratch/<username>``

-  scratch file system for simulation output and other temporary data

-  there are no backups for ``/scratch``: hardware error or human error
   can lead to data loss.

-  A per-user quota of (by default) 25TB is applied. This is in place to
   prevent jobs that (unintentionally) write arbitrary amount of data to
   ``/scratch`` from filling up the file system and blocking the system
   for everyone.

-  The following policy is applied to manage overall usage of
   ``/scratch``:

   If ``/scratch`` fills up, the cluster becomes unusable. Should this
   happen, we will make space available through the following actions:

   #. purchase and installation of additional hardware to increase
      storage available ``/scratch`` (if funding and other constraints
      allow this)

   #. ask users to voluntarily reduce their usage of ``/scratch`` (by,
      for example, deleting some data, or archiving completed projects
      elsewhere)

   #. if 1. and 2. do not resolve the situation, a script will be
      started that deletes some of the files on ``/scratch`` (starting
      with the oldest files). Notice will be given of this procedure.

.. note::

   The backup of the old HPC installation is available under
   ``/backup/<username>`` on the login nodes. Users can copy their existing data to
   the new home and scratch directories. No data will be transfered automatically.

.. _mpsd-hpc-software:

Software
--------

The software on the MPSD HPC system is provided via environment modules.
This facilitates providing different versions of the same software. The
software is organised in a hierarchical structure.

First, you need to decide which MPSD software environment **version**
you need. These are named according to calendar years: the first one in
2023 is ``23a``. In preparation of that version, there is a ``dev-23a``
which is used until ``23a`` is completed. In the examples below, this is
always ``dev-23a``. We select that version using the ``mpsd-modules``
command, for example ``mpsd-modules dev-23a``.

In order to use a module we first have to load a base compiler and MPI.
That way we can choose between different compilers and MPI
implementations for a software. More details are given below.

From a high-level perspective, the required steps to use a particular
module are:

#. Activate the MPSD software environment version of modules
#. Search for the module to find available versions and required base
   modules
#. Load required base modules (such as a compiler)
#. Load the desired module

TLDR
~~~~

-  Modules are organised in a hierarchical structure with compiler and
   MPI implementation as base modules.
-  Modules may be compiled with different feature sets. Use
   ``mpsd-modules`` for switching.

   -  a generic feature set (runs on all nodes), activated by default

      ::

         mpsd-modules dev-23a

   -  architecture-dependent feature sets (depending on the CPU micro
      architecture ``$MPSD_MICROARCH`` of the nodes)

      ::

         mpsd-modules -m $MPSD_MICROARCH dev-23a 

-  To subsequently find and load modules:

   -  ``module avail``
   -  ``module spider <module-name>``
   -  ``module load <module1> [<module2> ...]``

-  ``toolchain`` modules replicate `easybuild
   toolchains <https://docs.easybuild.io/en/latest/Common-toolchains.html>`__
   and additionally contain all modules required to compile octopus.
-  Configure scripts to compile octopus are available under
   ``/opt_mpsd/linux-debian11/dev-23a/spack-environments/octopus/``.

Initial setup
~~~~~~~~~~~~~

The MPSD HPC system consists of a heterogeneous set of compute nodes
with different CPU features. This is reflected in the available software
stack by providing both a *generic* set of modules that can be used on
all nodes as well as *specialised* sets of modules for the different
available (hardware) micro architectures. The latter will only run on
certain nodes.

A versioning scheme is used for the MPSD software environment to improve
reproducibility. Currently, all software is available in the ``dev-23a``
release (i.e. the first release in 2023). Additional modules will be
added to this environment as long as they do not break anything.
Therefore, users should always specify the version of the modules they
use (even if only a single version is available). A new release will be
made if any addition/change would break backwards compatibility.

This heterogeneous setup makes it necessary to first add an additional
path where module files can be found. To activate the different sets of
modules we can use ``mpsd_modules``. The function takes two arguments:
the *release number* (of the MPSD software environment, mandatory) and
the *feature set* (optional, the *generic* set is used by default).
Calling ``mpsd_modules`` without arguments will show help and list
available options.

To demonstrate the use of ``mpsd_modules`` we activate the ``generic``
module set of the current software environment ``dev-23a``. These
modules can be used on all HPC nodes.

.. code:: bash

   ~$ mpsd-modules dev-23a

Now, we can list available modules. At the time of writing this produces
the following output (truncated):

.. code:: bash

   ~$ module avail

   ------------ /opt_mpsd/linux-debian11/develop/sandybridge/lmod/Core -------------
      gcc/10.3.0                       toolchains/foss2021a-mpi
      toolchains/foss2021a-cuda-mpi    toolchains/foss2021a-serial (D)

   ----------------------- /usr/share/lmod/lmod/modulefiles -----------------------
      Core/lmod    Core/settarg (D)

   ------------------------ /usr/share/modules/modulefiles ------------------------
      mathematica    mathematica12p2    matlab    matlab2021b

     Where:
      D:  Default Module

We can only see a small number of modules. The reason for this is the
hierarchical structure mentioned before. The majority of modules are
only visible once we load a compiler (and depending on the package an
MPI implementation).

We can load a compiler and again list available modules. Now many more
are available:

.. code:: bash

   ~$ module load gcc/10.3.0
   ~$ module avail

   --------- /opt_mpsd/linux-debian11/develop/sandybridge/lmod/gcc/10.3.0 ----------
      autoconf-archive/2022.02.11               libsigsegv/2.13
      autoconf/2.71                             libtiff/4.4.0
      automake/1.16.3                           libtool/2.4.6
      bdftopcf/1.0.5                            libvdwxc/0.4.0
      berkeley-db/18.1.40                       libxc/4.3.4
      berkeleygw/2.1                            libxfont/1.5.2
      binutils/2.36.1                           libxml2/2.10.1
      bison/3.8.2                               lz4/1.9.4
      boost/1.80.0                              m4.1.1.19
      bzip2/1.0.8                               meson/0.63.3
      ca-certificates-mozilla/2022-10-11        metis/5.1.0
      cgal/5.0.3                                mkfontdir/1.0.7
      cmake/3.24.3                              mkfontscale/1.1.2
      cuda/11.4.4                               mpfr/4.1.0
      curl/7.85.0                               munge/0.5.15
      diffutils/3.8                             nasm/2.15.05
      eigen/3.4.0                               ncurses/6.3
      expat/2.4.8                               nfft/3.4.1
      fftw/3.3.9                                ninja/1.11.1
      findutils/4.9.0                           nlopt/2.7.0
      flex/2.6.3                                numactl/2.0.14
      flexiblas/3.0.4                           openmpi/4.1.1-cuda-11.4.4
      font-util/1.3.2                           openmpi/4.1.1             (D)
      fontconfig/2.13.94                        openssh/9.1p1
      fontsproto/2.1.3                          openssl/1.1.1s
      freetype/2.11.1                           pcre/8.45
      gawk/5.1.1                                pcre2/10.39
      gdbm/1.23                                 perl/5.36.0
      gdrcopy/2.3                               pigz/2.7
      gettext/0.21.1                            pkgconf/1.8.0
      glib/2.74.1                               pmix/4.1.2-cuda-11.4.4
      gmp/6.2.1                                 pmix/4.1.2                (D)
      gperf/3.1                                 py-cython/0.29.32
      gsl/2.7                                   py-docutils/0.19
      hdf5/1.12.2                               py-numpy/1.23.4
      hwloc/2.8.0-cuda-11.4.4                   py-pip/22.2.2
      hwloc/2.8.0                        (D)    py-setuptools/59.4.0
      json-c/0.16                               py-wheel/0.37.1
      knem/1.1.4-cuda-11.4.4                    python/3.9.5
      knem/1.1.4                         (D)    rdma-core/41.0
      krb5/1.19.3                               readline/8.1.2
      libbsd/0.11.5                             slurm/21-08-8-2
      libedit/3.1-20210216                      sparskit/develop
      libevent/2.1.12                           sqlite/3.39.4
      libffi/3.4.2                              swig/4.0.2
      libfontenc/1.1.3                          tar/1.34
      libfuse/3.11.0                            texinfo/6.5
      libgcrypt/1.10.1                          ucx/1.13.1-cuda-11.4.4
      libgd/2.2.4                               ucx/1.13.1                (D)
      libgpg-error/1.46                         util-linux-uuid/2.38.1
      libiconv/1.16                             util-macros/1.19.3
      libjpeg-turbo/2.1.3                       xproto/7.0.31
      libmd/1.0.4                               xtrans/1.3.5
      libnl/3.3.0                               xz/5.2.7
      libpciaccess/0.16                         zlib/1.2.13
      libpng/1.6.37                             zstd/1.5.2

   ------------ /opt_mpsd/linux-debian11/develop/sandybridge/lmod/Core -------------
      gcc/10.3.0                    (L)    toolchains/foss2021a-mpi
      toolchains/foss2021a-cuda-mpi        toolchains/foss2021a-serial (D)

   ----------------------- /usr/share/lmod/lmod/modulefiles -----------------------
      Core/lmod    Core/settarg (D)

   ------------------------ /usr/share/modules/modulefiles ------------------------
      mathematica (L)    mathematica12p2 (L)    matlab (L)    matlab2021b (L)

     Where:
      L:  Module is loaded
      D:  Default Module

We now unload all loaded modules:

.. code:: bash

   ~$ module purge

.. _loading_specific_packages:

Loading specific packages
~~~~~~~~~~~~~~~~~~~~~~~~~

To find a specific package we can use the ``module spider`` command.
Without extra arguments this would list all modules. We can search for a
specific module by adding the module name.

As an example we can search for ``octopus``:

.. code:: bash

   ~$ module spider octopus

We see that different versions of ``octopus`` are available. We have to
specify a particular version in order to get more information on how to
load the module:

.. code:: bash

   ~$ module spider octopus/12.1

We can see that we have to first load ``gcc/10.3.0`` and
``openmpi/4.1.1`` in order to be able to load and use ``octopus/12.1``.

.. note::

   Sometimes ``module spider`` will suggest to either only load a compiler or
   compiler + MPI implementation. Then, we generally want to also load the MPI
   implementation as only this version of the program will use MPI. Loading the MPI-enabled
   version of the desired program is crucial when running a slurm job on multiple nodes.

We load ``gcc/10.3.0``, ``openmpi/4.1.1`` and finally ``octopus/12.1``.
All of this can be done in one line as long as the packages are given in
the correct order (as shown by ``module spider``):

.. code:: bash

   ~$ module load gcc/10.3.0 openmpi/4.1.1 octopus/12.1

As a first simple check we display the version number of ``octopus``:

::

   ~$ octopus --version
   octopus 12.1 (git commit )

Python
~~~~~~

To use Python we load the ``anaconda3`` module:

.. code:: bash

   ~$ module load anaconda3/2022.05

Anaconda comes with a wide variety of pre-installed Python packages such
as ``numpy``, ``scipy``, ``matplotlib``, etc.

#. Numpy example

   We can execute a small demo program called ``hello-numpy.py``. The
   file has the following content.

   .. code:: python

      import numpy as np

      print("Hello world")
      print(f"numpy version: {np.__version__}")
      x = np.arange(5)
      y = x**2
      print(y)

   ::

      ~$ python3 hello_numpy.py
      Hello world
      numpy version: 1.21.5
      [ 0  1  4  9 16]

#. Custom conda environment

   We can also create a separate conda environment if we need additional
   Python software or different versions. We suggest to use *Miniconda*
   for this (as documented
   `here <https://conda.io/projects/conda/en/latest/user-guide/install/linux.html>`__
   in more detail).

   First, we have to download the Miniconda installer from
   https://docs.conda.io/en/latest/miniconda.html#linux-installers.
   Then, we can run the installer and follow the instructions.

   .. code:: bash

      ~$ wget https://repo.anaconda.com/miniconda/Miniconda3-py310_22.11.1-1-Linux-x86_64.sh
      ~$ bash Miniconda3-py310_22.11.1-1-Linux-x86_64.sh

   .. admonition:: Recommendations
      :class: hint

      - run ``conda init`` when asked about to get access to the conda
        executable and
      - remove the auto-activation of the base environment to avoid potential conflicts:
        ``conda config --set auto_activate_base false``

        (If you need to activate the base environment, use ``conda activate base``.)

   As an example we now create a new environment, called
   ``my_conda_env``, with an older version of Python and a specific
   numpy version from the ``conda-forge`` channel.

   ::

      ~$ conda create -n my_conda_env -c conda-forge python=3.9 numpy=1.23

   We can now activate the environment and check the versions of Python
   and numpy.

   ::

      ~$ conda activate my_conda_env
      ~$ python --version
      Python 3.9.16
      ~$ python -c "import numpy; print(numpy.__version__)"
      1.23.5

   We can deactivate and remove the environment using:

   ::

      ~$ conda deactivate my_conda_env
      ~$ conda env remove -n my_conda_env

   .. warning::

      The local conda executable becomes unusable if you load and unload the
      ``anaconda3`` module. If running into this problem start a new shell.

Jupyter notebooks
~~~~~~~~~~~~~~~~~

You can use a Jupyter notebook on a dedicated HPC node as follows:

#. Ensure you are at MPSD or have the DESY VPN set up.

#. Login to a login node (for example ``mosh mpsd-hpc-login1.desy.de``,
   `mosh <https://mosh.org/>`__ is recommended over ``ssh`` to avoid
   losing the session in case of short connection interruptions e.g. on
   WiFi)

#. Request a node for interactive use. For example, 1 node for 600
   minutes from the ``public`` partition:

   .. code:: bash

      salloc --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --time=06:00:00 -p public

#. You can install Jupyter yourself, or you activate an installed
   version with the following commands:

   .. code:: bash

      module load anaconda3

#. Limit numpy (and other libraries) to the available cores

   .. code:: bash

      export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

#. Start the Jupyter notebook (or Jupyter lab) server on that node with

   .. code:: bash

      jupyter-notebook --no-browser --ip=${HOSTNAME}.desy.de

   Watch the output displayed in your terminal. There is a line similar
   to this one:

   ``http://mpsd-hpc-ibm-055.desy.de:8888/?token=8814fea339b8fe7d3a52e7d03c2ce942a3f35c8c263ff0b8``

   which you can paste as a URL into your browser (on your
   laptop/Desktop), and you should be connected to the Notebook server
   on the compute node.

Matlab
~~~~~~

``module load matlab``

Loading a toolchain to compile octopus
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

There is also a set of special meta-modules, called *toolchains*. These
load groups of modules (e.g. compiler, MPI, blas, â€¦). The versions of
the individual packages follow the `easybuild
toolchains <https://docs.easybuild.io/en/latest/Common-toolchains.html>`__.
In addition to the set of modules defined by easybuild the toolchains on
the HPC system also contain all modules required to compile ``octopus``
from source.

Here, we show two examples how to compile ``octopus``, a serial and an
MPI version. Following this guide is only recommended if you need to
compile octopus from source. We also provide pre-compiled modules for
octopus as outlined in :ref:`loading_specific_packages` above.

As mentioned before, different variants of (most) modules are available
that support different CPU feature sets. So far we only discussed the
*generic* set that can be used on all nodes. In order to make use of all
available features on a specific node we can instead load a more
optimised set of modules. The CPU architecture is available in the
environment variable ``$MPSD_MICORARCH``.

First, we remove the *generic* module set and activate the optimised set
for the current node.

.. code:: bash

   ~$ module purge
   ~$ mpsd-modules -m $MPSD_MICROARCH dev-23a 

#. Parallel version of octopus

   We can load the toolchain ``foss2021a-mpi`` to compile ``octopus``
   using ``gcc 10.3.0`` and ``openmpi 4.1.1``:

   .. code:: bash

      ~$ module load toolchains/foss2021a-mpi

   Next, we clone octopus:

   .. code:: bash

      ~$ git clone https://gitlab.com/octopus-code/octopus.git
      ~$ cd octopus

   The SSU Computational science maintains a set of configure scripts
   that can be used to compile octopus with standard feature sets. These
   scripts are available at
   ``/opt_mpsd/linux-debian11/dev-23a/spack-environments/octopus/``. We
   use the following script (``foss2021a-mpi-config.sh``) for this
   compilation:

   .. literalinclude:: ../../external-sources/spack-environments/octopus/foss2021a-mpi-config.sh
      :language: bash

   Now, we can configure and compile octopus:

   .. code:: console

      ~$ autoreconf -i
      ~$ mkdir _build && cd _build
      ~$ cp PATH_TO_OCTOPUS_CONFIGURE_SCRIPTS/foss2021a-mpi-config.sh .
      ~$ source foss2021a-mpi-config.sh
      ~$ make
      ~$ make check-short

#. Serial version of octopus

   Compiling the serial version in principle consists of the same steps
   as the parallel version. We use a different toolchain and
   configuration script.

   We load the serial toolchain:

   .. code:: bash

      ~$ module load toolchains/foss2021a-serial

   and use the following configure script
   (``foss2021a-serial-config.sh``):

   .. literalinclude:: ../../external-sources/spack-environments/octopus/foss2021a-serial-config.sh
      :language: bash
      :lines: -20

   We can then follow the instructions from the previous section.

Compiling custom code
~~~~~~~~~~~~~~~~~~~~~

To compile other custom code we generally require a different collection
of modules than the one provided by the ``toolchains`` meta-modules. In
these cases it might be necessary to manually load all required modules.
The same general notes about *generic* and optimised module sets
explained in the previous section apply.

Here, we show two different examples. The sources are available under
``/opt_mpsd/examples/slurm-examples`` (TODO: not available at this
location yet, <2023-02-20 Mon>)

#. Serial "Hello world" in Fortran

   First, we want to compile the following "hello world" Fortran program
   using ``gcc``. We assume it is saved in a file ``hello.f90``. The
   source is available in
   ``/opt_mpsd/examples/slurm-examples/serial-fortran``.

   .. literalinclude:: ../../external-sources/mpsd-hpc-examples/slurm-examples/serial-fortran/hello.f90
      :language: bash

   We have to load ``gcc``:

   .. code:: bash

      module load gcc/10.3.0

   Then, we can compile and execute the program:

   .. code:: bash

      ~$ gfortran -o hello hello.f90
      ~$ ./hello
       Hello world!

#. MPI-parallelised "Hello world" in C

   As a second example we compile an MPI-parallelised "Hello world" C
   program, again using ``gcc``. We assume the source is saved in a file
   ``hello-mpi.c`` (source available under
   ``/opt_mpsd/examples/slurm-examples/mpi-c``).

   .. literalinclude:: ../../external-sources/mpsd-hpc-examples/slurm-examples/mpi-c/hello-mpi.c
      :language: bash

   We have to load ``gcc`` and ``openmpi``:

   .. code:: bash

      ~$ module load gcc/10.3.0 openmpi/4.1.1

   Now, we can compile and execute the test program:

   ::

      ~$ mpicc -o hello-mpi hello-mpi.c
      ~$ orterun -n 4 ./hello-mpi
      Hello world from rank 2 out of 4 on mpsd-hpc-login1.
      Hello world from rank 3 out of 4 on mpsd-hpc-login1.
      Hello world from rank 1 out of 4 on mpsd-hpc-login1.
      Hello world from rank 0 out of 4 on mpsd-hpc-login1.

   .. note::

      Inside a slurm job ``srun`` has to be used instead of ``orterun``.

   .. _examplebatchscripts:

Example batch scripts
---------------------

Here, we show a number of example batch scripts for different types of
jobs. All examples are available on the HPC system under
``/opt_mpsd/examples/slurm-examples`` together with the example
programs. (TODO: not available yet, <2023-02-20 Mon>) We use the
``mpsd`` partition and the *generic* module set for all examples.

To test an example on the HPC system we can copy the relevant directory
into our scratch directory. If required we can compile the code using
``make`` and then submit the job using ``sbatch submission-script.sh``.

MPI
~~~

The source code and submission script are in
``/opt_mpsd/examples/slurm-examples/mpi-c``.

.. literalinclude:: ../../external-sources/mpsd-hpc-examples/slurm-examples/mpi-c/submission-script.sh
   :language: bash

.. _example-openmp-mpi:

MPI + OpenMP
~~~~~~~~~~~~

The source code and submission script are in
``/opt_mpsd/examples/slurm-examples/mpi-openmp-c``.

.. literalinclude:: ../../external-sources/mpsd-hpc-examples/slurm-examples/mpi-openmp-c/submission-script.sh
   :language: bash

.. _example-openmp:

OpenMP
~~~~~~

The source code and submission script are in
``/opt_mpsd/examples/slurm-examples/openmp-c``.

.. literalinclude:: ../../external-sources/mpsd-hpc-examples/slurm-examples/openmp-c/submission-script.sh
   :language: bash

.. _example-python-multiprocessing:

Python with numpy or multiprocessing
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The source code and submission script are in
``/opt_mpsd/examples/slurm-examples/python-numpy``.

.. literalinclude:: ../../external-sources/mpsd-hpc-examples/slurm-examples/python-numpy/submission-script.sh
   :language: bash

.. _example-singlecore:

Single-core job
~~~~~~~~~~~~~~~

The source code and submission script are in
``/opt_mpsd/examples/slurm-examples/serial-fortran``.

.. literalinclude:: ../../external-sources/mpsd-hpc-examples/slurm-examples/serial-fortran/submission-script.sh
   :language: bash

Serial Python
~~~~~~~~~~~~~

The source code and submission script are in
``/opt_mpsd/examples/slurm-examples/python-serial``.

.. literalinclude:: ../../external-sources/mpsd-hpc-examples/slurm-examples/python-serial/submission-script.sh
   :language: bash

GPU jobs
~~~~~~~~

For GPU jobs, we recommend to specify desired hardware resources as
follows. In parenthesis we provide the typical application (MPI, OpenMP)
for guidance.

-  ``nodes`` - how many computers to use, for example ``--nodes=1``
-  ``tasks-per-node`` - how many (MPI) processed to run per node:
   ``--tasks-per-node=4``
-  ``gpus-per-task`` - how many GPUs per (MPI) process to use (often 1):
   ``--gpus-per-task=1``
-  ``cpus-per-task`` - how many CPUs (OpenMP threads) to use:
   ``--cpus-per-task=4``

Example:

.. code:: console

   user@mpsd-hpc-login1:~$ salloc --nodes=1 --tasks-per-node=4 --gpus-per-task=1 --cpus-per-task=4 --mem=128G -p gpu 
   user@mpsd-hpc-gpu-002:~$ mpsd-show-job-resources
      9352 Nodes: mpsd-hpc-gpu-002
      9352 Local Node: mpsd-hpc-gpu-002

      9352 CPUSET: 0-7,16-23
      9352 MEMORY: 131072 M

      9352 GPUs (Interconnects, CPU Affinity, NUMA Affinity):
      9352 GPU0     X  NV1 NV1 NV2 SYS 0-7,16-23   0-1
      9352 GPU1    NV1  X  NV2 NV1 SYS 0-7,16-23   0-1
      9352 GPU2    NV1 NV2  X  NV2 SYS 0-7,16-23   0-1
      9352 GPU3    NV2 NV1 NV2  X  SYS 0-7,16-23   0-1
   user@mpsd-hpc-gpu-002:~$

We can see from the output that we have one node (mpsd-hpc-gpu-002), 16
CPUs (with ids 0 to 7 and 16 to 23), 128GB (=131072MiB), and 4 GPUs
allocated (GPU0 to GPU3).

(TODO: Add CUDA Slurm example program and submission script.)
